{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "3b8db4a801e3eff5ba2f9266cff76cd73b19e0b2"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "958216cee1e8f878260be62ccec71aa452871f16"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "277abf3307bf4abe35ec4ae21650ab0cc430bdd8"
   },
   "outputs": [],
   "source": [
    "#from src.util import *\n",
    "#from src.data import *\n",
    "#from src.pmodel import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6054719f99c5cba8fd2130183642474cd8b1e4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle run\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "KAGGLE_RUN = (not os.path.exists('/opt/conda/home/.history'))\n",
    "if KAGGLE_RUN: print('Kaggle run')\n",
    "\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "\n",
    "def tf_seed_everything(seed):\n",
    "    import tensorflow as tf\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def f1_curve(target, preds, t_min=0.01, t_max=0.99, steps=99):\n",
    "    curve = {}\n",
    "    for t in np.linspace(t_min, t_max, steps):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            curve[t] = sklearn.metrics.f1_score(target, preds >= t)\n",
    "    return pd.Series(curve).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "517677d6446af0de10cb97877c87d6b0e8b9b155"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sklearn.preprocessing\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuoraData:\n",
    "    def __init__(self):\n",
    "        self.paths = {\n",
    "            'glove': '../input/embeddings/glove.840B.300d/glove.840B.300d.txt',\n",
    "            'news': '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin',\n",
    "            'paragram': '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt',\n",
    "            'wiki': '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec',\n",
    "        }\n",
    "\n",
    "    def glove(self): return QuoraEmbedding(self.paths['glove'])\n",
    "    def news(self): return QuoraEmbedding(self.paths['news'])\n",
    "    def paragram(self): return QuoraEmbedding(self.paths['paragram'])\n",
    "    def wiki(self): return QuoraEmbedding(self.paths['wiki'])\n",
    "\n",
    "    def convert_start(self, embeddings):\n",
    "        \"\"\"Start conversion of specified embedding names to .npy format in background.\"\"\"\n",
    "\n",
    "        self.convert_pids = []\n",
    "\n",
    "        for name in embeddings:\n",
    "            path = self.paths[name]\n",
    "            if path.endswith('.npy'): continue\n",
    "            if os.path.exists(path + '.npy'): continue\n",
    "            if os.path.exists(name + '.npy'): continue\n",
    "            if KAGGLE_RUN:\n",
    "                out_path = name + '.npy'\n",
    "            else:\n",
    "                out_path = path + '.npy'\n",
    "            print(f'Converting {path} -> {out_path}', flush=True)\n",
    "            pid = os.fork()\n",
    "            if pid == 0:\n",
    "                emb = QuoraEmbedding(path)\n",
    "                emb.save_npy(out_path)\n",
    "                os._exit(0)\n",
    "\n",
    "            self.convert_pids.append(pid)\n",
    "            self.paths[name] = out_path\n",
    "\n",
    "    def convert_wait(self):\n",
    "        \"\"\"Wait for .npy conversion to finish.\"\"\"\n",
    "        for pid in self.convert_pids:\n",
    "            try:\n",
    "                os.waitpid(pid, 0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def read_train(self):\n",
    "        return pd.read_csv('../input/train.csv')\n",
    "\n",
    "    def read_test(self):\n",
    "        return pd.read_csv('../input/test.csv')\n",
    "\n",
    "    def read_input(self):\n",
    "        return pd.concat([self.read_train(), self.read_test()], axis=0, copy=False, sort=False, ignore_index=True)\n",
    "\n",
    "\n",
    "class QuoraEmbedding:\n",
    "    \"\"\"Quora's pretrained embeddings loader.\"\"\"\n",
    "\n",
    "    def __init__(self, filename, vectors=None, vocab=None):\n",
    "        self.filename = filename\n",
    "        if vectors is None:\n",
    "            vectors, vocab = self._read(filename)\n",
    "        self.vectors = vectors\n",
    "        self.index2word = vocab\n",
    "        self.name = re.findall('^\\w+', os.path.basename(filename))[0]\n",
    "        self.shape = self.vectors.shape\n",
    "        self.num_words, self.dim = self.shape\n",
    "        assert len(self.index2word) == self.num_words\n",
    "        # On collisions, pick the earliest (more frequent) vector's index\n",
    "        self.word2index = { self.index2word[i]: i for i in reversed(range(self.num_words)) }\n",
    "        self.lword2index = { self.index2word[i].lower(): i for i in reversed(range(self.num_words)) }\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'QuoraEmbedding({self.name}, {self.shape[0]}x{self.shape[1]}, {self.vectors.nbytes/(1024**3):.1f}GiB)'\n",
    "\n",
    "    def lookup(self, word, lower=False):\n",
    "        if lower:\n",
    "            idx = self.lword2index.get(word.lower(), -1)\n",
    "        else:\n",
    "            idx = self.word2index.get(word, -1)\n",
    "        return self.vectors[idx] if idx != -1 else None\n",
    "\n",
    "    @functools.lru_cache(1)\n",
    "    def mean(self): return self.vectors.mean()\n",
    "\n",
    "    @functools.lru_cache(1)\n",
    "    def std(self): return self.vectors.std()\n",
    "\n",
    "    def save_npy(self, filename):\n",
    "        assert filename.endswith('.npy')\n",
    "        np.save(filename, self.vectors)\n",
    "        with open(filename[:-4] + '.vocab', 'wb') as fp:\n",
    "            pickle.dump(self.index2word, fp)\n",
    "\n",
    "    @classmethod\n",
    "    def _read(cls, filename):\n",
    "        \"\"\"Reads file, returns (weights matrix, vocabulary list).\"\"\"\n",
    "        if filename.endswith('.npy'):\n",
    "            return cls._read_npy(filename)\n",
    "        elif os.path.exists(filename + '.npy'):\n",
    "            return cls._read_npy(filename + '.npy')\n",
    "        elif os.path.exists(re.sub('[.]\\w+$', '.npy', filename)):\n",
    "            return cls._read_npy(re.sub('[.]\\w+$', '.npy', filename))\n",
    "        elif filename.endswith('.bin'):\n",
    "            return cls._read_bin(filename)\n",
    "        else:\n",
    "            return cls._read_txt(filename)\n",
    "\n",
    "    @classmethod\n",
    "    def _read_npy(cls, filename):\n",
    "        assert filename.endswith('.npy')\n",
    "        vectors = np.load(filename, 'r')\n",
    "        with open(filename[:-4] + '.vocab', 'rb') as fp:\n",
    "            vocab = pickle.load(fp)\n",
    "        return vectors, vocab\n",
    "\n",
    "    @classmethod\n",
    "    def _read_txt(cls, filename):\n",
    "        vectors = []\n",
    "        vocab = []\n",
    "        dim = -1\n",
    "\n",
    "        for line_num, line in enumerate(open(filename, 'rb')):\n",
    "            try:\n",
    "                line = line.decode('utf-8')\n",
    "            except:\n",
    "                line = line.decode('latin_1')\n",
    "                print(f'Bad word on line {line_num+1}: {line[:20]}...')\n",
    "\n",
    "            word, line = line.split(' ', maxsplit=1)\n",
    "            vec = np.fromstring(line, np.float32, count=dim, sep=' ')\n",
    "            if len(vec) == 1 and len(vectors) == 0:\n",
    "                dim = int(vec[0])\n",
    "                continue\n",
    "\n",
    "            if dim != -1 and dim != len(vec):\n",
    "                raise Exception(f'Mismatching vector lengths: {dim} vs {len(vec)}')\n",
    "            dim = len(vec)\n",
    "\n",
    "            vectors.append(vec)\n",
    "            vocab.append(word)\n",
    "\n",
    "        vectors = np.stack(vectors)\n",
    "        vocab = '\\n'.join(vocab).split('\\n')\n",
    "        return vectors, vocab\n",
    "\n",
    "    @classmethod\n",
    "    def _read_bin(cls, filename):\n",
    "        from gensim.models import KeyedVectors\n",
    "        kv = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "        return kv.vectors, kv.index2word\n",
    "\n",
    "\n",
    "class QuoraPreprocessor:\n",
    "    \"\"\"Preprocesses raw text before tokenization.\"\"\"\n",
    "\n",
    "    punct = r\"\"\"-!\"#$%^&*+,.'\\\\\\[\\]()/:;?@_{}|~`’:”“=…<>√£°₹×€—？÷र™−•¿→®一，¹²³⁴₂∞℅∫∆øΔ∈½·≠（）。»«ʻくº—\"\"\"\n",
    "    punct += \"\\xa0\"\n",
    "\n",
    "    specialLR = '|'.join([re.sub('([.()])', r'\\\\\\1', s) for s in r\"\"\"\n",
    "        's 'm 'd 'll 're 've n't 'em o'clock\n",
    "        i.e. e.g. vs. U.S. U.K. [A-Za-z]. a.m. p.m.\n",
    "        e-mail t-shirt\n",
    "        : :) :D :-) :) ;) :-) =) ;-)\n",
    "        [0-9]+,[0-9]{3}\n",
    "    \"\"\".split()])\n",
    "\n",
    "    specialL = '|'.join([re.sub('([.()])', r'\\\\\\1', s) for s in r\"\"\"\n",
    "        [A-Za-z]. Mr. Mrs. Dr. a.m. p.m.\n",
    "    \"\"\".split()])\n",
    "\n",
    "    special = '|'.join([s for s in r\"\"\"\n",
    "        \\.{3,5}  [?!]{1,3}\n",
    "    \"\"\".split()])\n",
    "\n",
    "    re_ws = re.compile(r'[\\t\\n ]+')\n",
    "    re_apos = re.compile(r\"\"\"['’`‘´`′′]\"\"\")\n",
    "    re_contractions = re.compile(r\"(\\w)('s|'m|'d|'ll|'re|'ve|n't|'em)( |[%s])\" % punct, re.UNICODE | re.I)\n",
    "    re_special = re.compile(fr\"(((?<= )({specialLR})(?= ))|((?<= )({specialL}))|{special}|[{punct}])\",\n",
    "                            re.UNICODE | re.I)\n",
    "\n",
    "    tests = [\n",
    "        (\"Don`t y'all thinkin' it's, like, we're getting... a\\xa0little ``too'' late?!?!\",\n",
    "         \"Do n't y ' all thinkin ' it 's , like , we 're getting ... a \\xa0 little ' ' too ' ' late ?!? !\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        for inp, exp in self.tests:\n",
    "            outp = self.transform_str(inp)\n",
    "            assert outp == exp, (outp, exp)\n",
    "\n",
    "    def transform_str(self, text):\n",
    "        text = ' ' + self.re_ws.sub(' ', text) + ' '\n",
    "        text = self.re_apos.sub(\"'\", text)\n",
    "        text = self.re_contractions.sub(r'\\1 \\2 \\3', text)\n",
    "        text = self.re_special.sub(r' \\1 ', text)\n",
    "        text = self.re_ws.sub(' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def transform(self, texts, n_jobs=None, chunksize=10000):\n",
    "        if type(texts) is str:\n",
    "            return self.transform_str(texts)\n",
    "        if len(texts) < chunksize:\n",
    "            return [self.transform_str(s) for s in texts]\n",
    "        if n_jobs is None:\n",
    "            n_jobs = min(20, multiprocessing.cpu_count())\n",
    "        with multiprocessing.Pool(n_jobs) as pool:\n",
    "            res = list(pool.imap(self.transform_str, texts, chunksize=chunksize))\n",
    "            return pd.Series(res, dtype='O')\n",
    "\n",
    "\n",
    "class QuoraFeatureExtractor:\n",
    "    def __init__(self, num_words=120000, max_len=70, want_capstate=False, want_wide=False, preprocessor=None):\n",
    "        self.max_len = max_len\n",
    "        self.num_words = num_words\n",
    "        self.num_aux = 1\n",
    "        self.preprocessor = preprocessor\n",
    "        self.want_capstate = want_capstate\n",
    "        self.want_wide = want_wide\n",
    "\n",
    "    def fit_transform(self, texts, fit_mask=None):\n",
    "        if fit_mask is None:\n",
    "            fit_mask = np.full(len(texts), True, dtype='bool')\n",
    "\n",
    "        if self.preprocessor:\n",
    "            print('Preprocessing', flush=True)\n",
    "            texts = self.preprocessor.transform(texts)\n",
    "            fit_mask = np.array(fit_mask, dtype='bool')\n",
    "\n",
    "        print('Tokenizing', flush=True)\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=self.num_words, filters='', split=' ', oov_token='__')\n",
    "        self.tokenizer.fit_on_texts(texts[fit_mask])\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        tokens = self.tokenizer.texts_to_sequences(texts)\n",
    "        tokens = pad_sequences(tokens, maxlen=self.max_len)\n",
    "        res = {'tokens': tokens}\n",
    "\n",
    "        if self.want_capstate:\n",
    "            print('Generating capstate', flush=True)\n",
    "            with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "                capstate = np.stack(pool.imap(self.map_capstate, texts, chunksize=10000))\n",
    "                res['capstate'] = capstate\n",
    "\n",
    "        if self.want_wide:\n",
    "            print('Generating wide features', flush=True)\n",
    "            wide = self.gen_wide_features(texts)\n",
    "            self.ss = sklearn.preprocessing.RobustScaler()\n",
    "            self.ss.fit(wide[fit_mask].values)\n",
    "            wide = self.ss.transform(wide.values)\n",
    "            wide[wide > 3] = 3\n",
    "            res['wide'] = wide\n",
    "\n",
    "        return res\n",
    "\n",
    "    def map_capstate(self, text):\n",
    "        toks = text.split(' ')\n",
    "        res = np.zeros(self.max_len, dtype=np.int8)\n",
    "\n",
    "        for i in range(min(len(res), len(toks))):\n",
    "            tok = toks[len(toks) - 1 - i]\n",
    "            x = 0\n",
    "            if tok[0].isdigit():\n",
    "                x = 6\n",
    "            elif unicodedata.category(tok[0]).startswith('P'):\n",
    "                x = 5\n",
    "            elif tok == tok.lower():\n",
    "                x = 1\n",
    "            elif tok == tok.capitalize():\n",
    "                x = 2\n",
    "            elif tok == tok.upper():\n",
    "                x = 3\n",
    "            else:\n",
    "                x = 4\n",
    "            res[-i-1] = x\n",
    "        return res\n",
    "\n",
    "    def gen_wide_features(self, texts):\n",
    "        texts = pd.Series(texts)\n",
    "        wide = pd.DataFrame()\n",
    "        wide['num_chars'] = texts.str.len()\n",
    "        wide['num_caps'] = texts.str.count('[A-Z]')\n",
    "        wide['frac_caps'] = (wide.num_caps / wide.num_chars).fillna(0)\n",
    "        wide['num_dots'] = texts.str.count('[.]')\n",
    "        #wide['num_words'] = texts.str.count(' ')\n",
    "        #wide['num_unique_words'] = texts.str.split(' ', n=70, expand=True).nunique(axis=1)\n",
    "        #wide['num_unique_words'] = np.minimum(wide['num_words'], wide['num_unique_words'])\n",
    "        #wide['frac_unique'] = (wide.num_unique_words / wide.num_words).fillna(0)\n",
    "        return wide\n",
    "\n",
    "    def embedding_weights(self, emb, verbose=1):\n",
    "        \"\"\"Embeds tokens with given pretrained embedding, initializes missing with random vectors.\"\"\"\n",
    "\n",
    "        W = np.random.normal(emb.mean(), emb.std(), (self.num_words, emb.dim))\n",
    "        #W[0, :] = 0\n",
    "        miss = 0\n",
    "        for word, idx in self.word_index.items():\n",
    "            if idx >= self.num_words: continue\n",
    "            vec = emb.lookup(word, lower=True)\n",
    "            if vec is not None:\n",
    "                W[idx] = vec\n",
    "            else:\n",
    "                miss += 1\n",
    "        if verbose > 0:\n",
    "            print(f'{miss} words from tokenizer missing ({miss/self.num_words:.2f}%) in {emb.name}')\n",
    "        return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3426abd1335b8672a859ad85fa808dd3bbe56154"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def torch_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def torch_gc():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def keras_rnn_init(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param)\n",
    "        if 'weight_hh' in name: torch.nn.init.orthogonal_(param)\n",
    "        if 'bias_' in name: torch.nn.init.constant_(param, 0)\n",
    "\n",
    "\n",
    "class Fold:\n",
    "    def __init__(self, data, X, seed=42, valid_frac=0.1, holdout_frac=0.0432, holdout_seed=42):\n",
    "        self.data = data\n",
    "        self.X = X\n",
    "        self.y = data.target.values.astype(np.float32)\n",
    "        self.seed = seed\n",
    "\n",
    "        tmask = data.target.notnull().values\n",
    "\n",
    "        self.all_idx = data.index.values.astype(np.int32)\n",
    "        self.test_idx = self.all_idx[~tmask]\n",
    "\n",
    "        # Split off holdout sample\n",
    "        trainval_idx, self.holdout_idx = train_test_split(\n",
    "            self.all_idx[tmask],\n",
    "            stratify=self.y[tmask],\n",
    "            test_size=holdout_frac,\n",
    "            random_state=holdout_seed,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.train_idx, self.valid_idx = train_test_split(\n",
    "            trainval_idx,\n",
    "            stratify=self.y[trainval_idx],\n",
    "            test_size=valid_frac,\n",
    "            random_state=seed,\n",
    "            shuffle=True)\n",
    "\n",
    "        self.train_idx.sort()\n",
    "        self.valid_idx.sort()\n",
    "        self.holdout_idx.sort()\n",
    "        self.oob_idx = self.all_idx[~np.isin(self.all_idx, self.train_idx, assume_unique=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f16c9f4cebf8a4da1abb4dd4b3a84d8220635f1"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26d9dc5ec332ad0be2045872eafc7341f8ba90a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ../input/embeddings/glove.840B.300d/glove.840B.300d.txt -> glove.npy\n",
      "Converting ../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec -> wiki.npy\n",
      "CPU times: user 5.48 s, sys: 744 ms, total: 6.22 s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_seed(42)\n",
    "\n",
    "qd = QuoraData()\n",
    "qd.convert_start(['glove', 'wiki'])\n",
    "\n",
    "prep = QuoraPreprocessor()\n",
    "input_df = qd.read_input()\n",
    "input_df['question_text'] = prep.transform(input_df.question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "94371f4b4f519bb9dea807f8f0142be1a76009c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': (1362492, 70)}\n",
      "CPU times: user 1min 26s, sys: 1.48 s, total: 1min 28s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qfe = QuoraFeatureExtractor(num_words=95000, max_len=70)\n",
    "input_X = qfe.fit_transform(input_df.question_text, fit_mask=input_df.target.notnull().values)\n",
    "print({ k: v.shape for (k, v) in input_X.items() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c45722e5a16e0d02a3ba2da8678924ec583b4825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6366 words from tokenizer missing (0.07%) in glove\n",
      "9192 words from tokenizer missing (0.10%) in wiki\n",
      "CPU times: user 10.3 s, sys: 3.15 s, total: 13.4 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qd.convert_wait()\n",
    "\n",
    "torch_seed(42)\n",
    "emb_glove = qfe.embedding_weights(qd.glove())\n",
    "\n",
    "torch_seed(43)\n",
    "emb_wiki = qfe.embedding_weights(qd.wiki())\n",
    "\n",
    "emb_glovewiki = np.concatenate([emb_glove, emb_wiki], axis=1)\n",
    "\n",
    "gc.collect()\n",
    "os.system('rm -f glove.npy glove.vocab wiki.npy wiki.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c9cdebf6a10dc0bdd34f2038f80e942bbd63255f"
   },
   "outputs": [],
   "source": [
    "class Arch1b_LstmGru(torch.nn.Module):\n",
    "    def __init__(self, emb_weights):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(emb_weights.shape[0], emb_weights.shape[1])\n",
    "        self.embedding.weight = nn.Parameter(torch.Tensor(emb_weights), requires_grad=False)\n",
    "        self.lstm = nn.LSTM(self.embedding.embedding_dim, 128, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(self.lstm.hidden_size*2, 64, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def dropout1d(self, x, p):\n",
    "        # x = [batch, time, channels]\n",
    "        x = x.permute(0, 2, 1)   # [batch, channels, time]\n",
    "        x = F.dropout2d(x, p, training=self.training)\n",
    "        x = x.permute(0, 2, 1)   # [batch, time, channels]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_tok):\n",
    "        x = self.embedding(x_tok)  # [batch, time, channels]\n",
    "        x = self.dropout1d(x, 0.2)\n",
    "        x = F.dropout2d(x, 0.05, training=self.training)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, c = self.gru(x)\n",
    "        x = x.max(1)[0]\n",
    "        x = F.dropout(x, 0.1, training=self.training)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "8749b81041bd41dc2c0bcf358f5eb2e8ac393c75"
   },
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, fold, model, seed=None, batch_size=512, grad_clip=1):\n",
    "        self.__dict__.update(fold.__dict__)\n",
    "        self.fold = fold\n",
    "        self.model = model\n",
    "\n",
    "        self.dataset = torch.utils.data.TensorDataset(\n",
    "            torch.LongTensor(self.X['tokens']),\n",
    "            #torch.FloatTensor(self.X['wide']),\n",
    "            torch.FloatTensor(self.y[:, np.newaxis]))\n",
    "        self.train_dl = self.make_loader(self.train_idx, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_dl = self.make_loader(self.valid_idx)\n",
    "\n",
    "        opt_params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.opt = torch.optim.Adam(opt_params)\n",
    "        self.grad_clip = grad_clip\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        self.steps = 0\n",
    "        self.epochs = 0\n",
    "\n",
    "        params_sz = sum([p.detach().cpu().numpy().nbytes for p in opt_params])\n",
    "        print(f'Training on {len(self.train_dl.dataset)} examples, '\n",
    "              f'validating on {len(self.valid_dl.dataset)} examples. '\n",
    "              f'Parameters: {params_sz/1048576:.2f}MiB', flush=True)\n",
    "\n",
    "        self.epoch_params = [ self.get_params() ]\n",
    "        self.val_metrics = []\n",
    "\n",
    "    def make_loader(self, idx, batch_size=1024, shuffle=False):\n",
    "        ds = torch.utils.data.Subset(self.dataset, idx)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            ds, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "    def get_params(self):\n",
    "        return { name: param.detach().cpu().numpy().copy()\n",
    "                 for (name, param) in self.model.named_parameters()\n",
    "                 if param.requires_grad }\n",
    "\n",
    "    def restore_params(self, prev):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in prev:\n",
    "                param.data.copy_(torch.from_numpy(prev[name]))\n",
    "\n",
    "    def restore_epoch(self, epoch):\n",
    "        self.restore_params(self.epoch_params[epoch])\n",
    "\n",
    "    def predict(self, idx=None, dl=None):\n",
    "        if dl is None:\n",
    "            dl = self.make_loader(idx, shuffle=False)\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for batch in dl:\n",
    "                batch = [t.cuda() for t in batch[:-1]]\n",
    "                pred = torch.sigmoid(self.model(*batch))\n",
    "                preds.append(pred.cpu().numpy().flatten())\n",
    "            self.model.train()\n",
    "\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "    def eval(self):\n",
    "        y_pred = self.predict(dl=self.valid_dl)\n",
    "        y_true = self.y[self.valid_idx]\n",
    "        curve = f1_curve(y_true, y_pred)\n",
    "        metrics = {\n",
    "            'epochs': self.epochs,\n",
    "            'steps': self.steps,\n",
    "            'f1': curve.max(),\n",
    "            'roc-auc': sklearn.metrics.roc_auc_score(y_true, y_pred),\n",
    "            'pr-auc': sklearn.metrics.average_precision_score(y_true, y_pred),\n",
    "            'loss': sklearn.metrics.log_loss(y_true, y_pred),\n",
    "            'thresh': curve.idxmax(),\n",
    "        }\n",
    "        self.val_metrics.append(metrics)\n",
    "        print(' '.join('%s=%.6g' % (k, float(v)) for (k, v) in metrics.items()), flush=True)\n",
    "\n",
    "    def train(self, epochs=1, eval_steps=0):\n",
    "        self.model.cuda()\n",
    "        self.model.train()\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            t_start = time.time()\n",
    "\n",
    "            if not KAGGLE_RUN and eval_steps > 0:\n",
    "                it = enumerate(tqdm_notebook(self.train_dl, leave=False))\n",
    "            else:\n",
    "                it = enumerate(self.train_dl)\n",
    "\n",
    "            for i, batch in it:\n",
    "                batch = [t.cuda() for t in batch]\n",
    "                y_batch = batch.pop()\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                y_pred = self.model(*batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                if self.grad_clip is not None:\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.steps += 1\n",
    "\n",
    "                if eval_steps and i % eval_steps == 0 and i > 0: self.eval()\n",
    "\n",
    "            self.epoch_params.append(self.get_params())\n",
    "            self.epochs += 1\n",
    "\n",
    "            print('t=%.2fs' % (time.time() - t_start), end=' ', flush=True)\n",
    "            self.eval()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "96a1aa0c3c0ecc2b284e17aaa84a998db3d61e98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1124727 examples, validating on 124970 examples. Parameters: 3.32MiB\n",
      "t=322.57s epochs=1 steps=2197 f1=0.669568 roc-auc=0.96367 pr-auc=0.6965 loss=0.101724 thresh=0.36\n",
      "t=320.69s epochs=2 steps=4394 f1=0.682056 roc-auc=0.966918 pr-auc=0.712921 loss=0.0985111 thresh=0.29\n",
      "t=319.40s epochs=3 steps=6591 f1=0.689178 roc-auc=0.968541 pr-auc=0.720979 loss=0.0968999 thresh=0.33\n",
      "t=319.64s epochs=4 steps=8788 f1=0.691279 roc-auc=0.969826 pr-auc=0.725795 loss=0.0953343 thresh=0.33\n",
      "t=319.66s epochs=5 steps=10985 f1=0.693596 roc-auc=0.97032 pr-auc=0.725591 loss=0.0957255 thresh=0.3\n",
      "CPU times: user 18min 5s, sys: 9min 31s, total: 27min 37s\n",
      "Wall time: 27min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold = Fold(input_df, input_X, seed=1005); torch_seed(fold.seed)\n",
    "learn1 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7bc99e9ae7cd1d567d0fcfff04261e3f2151a13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1124727 examples, validating on 124970 examples. Parameters: 3.32MiB\n",
      "t=319.93s epochs=1 steps=2197 f1=0.678604 roc-auc=0.965567 pr-auc=0.703658 loss=0.100478 thresh=0.41\n",
      "t=319.80s epochs=2 steps=4394 f1=0.688173 roc-auc=0.969085 pr-auc=0.719504 loss=0.0957745 thresh=0.33\n",
      "t=319.67s epochs=3 steps=6591 f1=0.692582 roc-auc=0.970748 pr-auc=0.727673 loss=0.0940008 thresh=0.31\n",
      "t=319.88s epochs=4 steps=8788 f1=0.697114 roc-auc=0.972026 pr-auc=0.732614 loss=0.0931273 thresh=0.31\n",
      "t=319.89s epochs=5 steps=10985 f1=0.700617 roc-auc=0.971837 pr-auc=0.731609 loss=0.0940498 thresh=0.41\n",
      "CPU times: user 18min, sys: 9min 31s, total: 27min 31s\n",
      "Wall time: 27min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold = Fold(input_df, input_X, seed=555); torch_seed(fold.seed)\n",
    "learn2 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "31ceca23335296ea83ff751863f1b096c36b39de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1124727 examples, validating on 124970 examples. Parameters: 3.32MiB\n",
      "t=320.09s epochs=1 steps=2197 f1=0.677603 roc-auc=0.964954 pr-auc=0.706662 loss=0.10055 thresh=0.31\n",
      "t=320.25s epochs=2 steps=4394 f1=0.691966 roc-auc=0.968738 pr-auc=0.726038 loss=0.0953433 thresh=0.36\n",
      "t=319.67s epochs=3 steps=6591 f1=0.69947 roc-auc=0.970003 pr-auc=0.731796 loss=0.0941457 thresh=0.38\n",
      "t=319.83s epochs=4 steps=8788 f1=0.699473 roc-auc=0.970755 pr-auc=0.734961 loss=0.0931271 thresh=0.35\n",
      "t=319.90s epochs=5 steps=10985 f1=0.700508 roc-auc=0.971183 pr-auc=0.735088 loss=0.0939119 thresh=0.36\n",
      "CPU times: user 18min 2s, sys: 9min 29s, total: 27min 32s\n",
      "Wall time: 27min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n",
    "learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "31ceca23335296ea83ff751863f1b096c36b39de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1124727 examples, validating on 124970 examples. Parameters: 3.32MiB\n",
      "t=319.81s epochs=1 steps=2197 f1=0.679796 roc-auc=0.965234 pr-auc=0.711333 loss=0.0999234 thresh=0.29\n",
      "t=319.92s epochs=2 steps=4394 f1=0.694737 roc-auc=0.968594 pr-auc=0.728012 loss=0.0960771 thresh=0.3\n",
      "t=319.54s epochs=3 steps=6591 f1=0.697179 roc-auc=0.970103 pr-auc=0.731373 loss=0.0941226 thresh=0.33\n",
      "t=319.68s epochs=4 steps=8788 f1=0.699915 roc-auc=0.970734 pr-auc=0.735654 loss=0.0937977 thresh=0.36\n",
      "t=319.79s epochs=5 steps=10985 f1=0.701904 roc-auc=0.970523 pr-auc=0.738312 loss=0.0943092 thresh=0.37\n",
      "CPU times: user 18min 1s, sys: 9min 30s, total: 27min 31s\n",
      "Wall time: 27min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fold = Fold(input_df, input_X, seed=3333); torch_seed(fold.seed)\n",
    "learn4 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68ed45e3c130456302f420f942dbee6a046582a5"
   },
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7c015525f77a64d99d06f80c7f7633a5a81395c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn1 ep5 0.699591\n",
      "learn2 ep5 0.699704\n",
      "learn3 ep5 0.696760\n",
      "learn4 ep5 0.693337\n",
      "CPU times: user 27.1 s, sys: 14.7 s, total: 41.8 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_idx = fold.test_idx\n",
    "hold_idx = fold.holdout_idx\n",
    "hold_y = input_df.loc[fold.holdout_idx, 'target']\n",
    "\n",
    "ensemble_hold = []\n",
    "ensemble_test = []\n",
    "for li, learn in enumerate([learn1, learn2, learn3, learn4]):\n",
    "    for ep in [5]:\n",
    "        if ep >= len(learn.epoch_params): continue\n",
    "        learn.restore_epoch(ep)\n",
    "        ensemble_hold.append(learn.predict(idx=hold_idx))\n",
    "        f1 = f1_curve(hold_y, ensemble_hold[-1]).max()\n",
    "        print('learn%d ep%d %.6f' % (li+1, ep, f1))\n",
    "        ensemble_test.append(learn.predict(idx=test_idx))\n",
    "\n",
    "ensemble_test_s = pd.Series(np.mean(ensemble_test, axis=0), index=fold.data.loc[test_idx, 'qid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "2c401d804c27e1e296bbf034b743960958c11f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7078178110129164, 0.37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve = f1_curve(hold_y, np.mean(ensemble_hold, axis=0))\n",
    "thresh = curve.idxmax()\n",
    "curve.max(), thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1cc54d2b48d2b2a17622b9e7fc1c9a6d9e3b88f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff8df11aa20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "curve.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e64f0b379d8a604a74d4ebff27da71c51686fe45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06785524215007983"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
    "submission_df['prediction'] = (ensemble_test_s.loc[submission_df.qid] >= thresh).astype(int).values\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.prediction.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
