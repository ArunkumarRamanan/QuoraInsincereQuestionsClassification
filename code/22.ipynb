{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d929dee7aa5b533cc1480f1a11e5e0722da25b32"
   },
   "source": [
    "Our solution utilizes some tricks that were public and some that we kept secret. Please don't use these in the future. You guys are making it too hard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "seed_nb=16\n",
    "import numpy as np \n",
    "np.random.seed(seed_nb)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_nb)\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import string\n",
    "import re\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.metrics import f1_score as off1\n",
    "#from sklearn.metrics import log_loss\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import h5py\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = '../input/'\n",
    "print(os.listdir(INPUT_PATH))\n",
    "debug = False\n",
    "do_submit = True\n",
    "simulate_test = False\n",
    "opt = \"val_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "620fbcb89aaab6fc0574da0871ae4e8268f9ee41"
   },
   "source": [
    "One thing we did along the way was test out how our model would perform with a much larger testing set. We did this simply by loading in the test set multiple times and concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3eced255e6b4a85b1cef5e67f7a00d4dd40899ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "(375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(INPUT_PATH + 'train.csv')\n",
    "# if submit use original test set else split off from train\n",
    "if simulate_test:\n",
    "    train_df, test_df = train_test_split(train_df, test_size = 0.1, random_state = 28)\n",
    "else:\n",
    "    test_df = pd.read_csv(INPUT_PATH + 'test.csv')\n",
    "#     for i in range(6):\n",
    "#         test_df = pd.concat([pd.read_csv(INPUT_PATH + 'test.csv'), test_df], axis = 0)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18a7f488d3c82a45f8d38adb2a69e70ed69a9fad"
   },
   "source": [
    "As most others have already shown in their strong placing kernels, preprocessing was very important. I think we probably could have done better here. It looks like many of the lemmatizing and tokenizing tricks we wrote off were actually useful. You can see how we went through a progression of text preprocessing based off of what is commented out. For a long time we were using the preprocessing that was publicly available in @dieters kernel https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings. Eventually we settled on the below preprocessing by systematically testing each option  and combination of options and seeing how they performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "47b691a4526c573ef130f40eec8f89b378f83395"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "# constants\n",
    "\n",
    "# from https://stackoverflow.com/questions/2013451/test-if-string-contains-only-letters-a-z-é-ü-ö-ê-å-ø-etc\n",
    "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
    "white_list = string.ascii_letters + string.digits + latin_similar + ' '\n",
    "# from checking symbols_iv = {char:char_vocab[char] for char in symbols if char in embeddings_index}\n",
    "symbols_iv = \"\"\"?,./-()\"$=…*&+′[ɾ̃]%:^\\xa0\\\\{}–“”;!<`®ạ°#²|~√_α→>—£，。´×@π÷？ʿ€の↑∞ʻ℅в•−а年！∈∩⊆§℃θ±≤͡⁴™си≠∂³ி½△¿¼∆≥⇒¬∨∫▾Ω＾γµº♭ー̂ɔ∑εντσ日Γ∪φβ¹∘¨″⅓ɑː✅✓（）∠«»்ுλ∧∀،＝ɨʋδɒ¸☹μΔʃɸηΣ₅₆◦·ВΦ☺❤♨✌≡ʌʊா≈⁰‛：ﬁ„¾ρ⟨⟩˂⅔≅－＞¢⁸ʒは⬇♀؟¡⋅ɪ₁₂ɤ◌ʱ、▒ْ；☉＄∴✏ωɹ̅।ـ☝♏̉̄♡₄∼́̀⁶⁵¦¶ƒˆ‰©¥∅・ﾟ⊥ª†ℕ│ɡ∝♣／☁✔❓∗➡ℝ位⎛⎝¯⎞⎠↓ɐ∇⋯˚⁻ˈ₃⊂˜̸̵̶̷̴̡̲̳̱̪̗̣̖̎̿͂̓̑̐̌̾̊̕\\x92\"\"\"\n",
    "\n",
    "def get_chars(train_X):\n",
    "    char_vocab = {}\n",
    "    for sent in tqdm_notebook(train_X):\n",
    "        for char in sent:\n",
    "            try:\n",
    "                char_vocab[char] += 1\n",
    "            except KeyError:\n",
    "                char_vocab[char] = 1\n",
    "    return char_vocab\n",
    "\n",
    "def split_off_symbols_iv(x):\n",
    "    for punct in symbols_iv:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = split_off_symbols_iv(text) #increase score\n",
    "    #text = delete_oov_symbols(text,translate_map)\n",
    "    #text = ' '.join(text.split())\n",
    "    #text = clean_apostrophes(text)\n",
    "    #text = clean_contractions(text, contraction_mapping)\n",
    "    #text = split_off_s(text)\n",
    "    #text = replace_acronyms(text,acronyms)\n",
    "    ##text = fix_mispells(text,mispells) decrease score\n",
    "    #text = fix_apo(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b790c0ebee70bd43431fe1e3fe950b7903ebb2d0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734857374ad140fb9b4c3c61deb1a276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "char_vocab = get_chars(train_df[\"question_text\"].values)\n",
    "symbols = {char:char_vocab[char] for char in char_vocab if not char in white_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8ba91019507ff2e836c1af64bd9f3db5be7eb528"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [01:47<00:00, 12112.63it/s]\n",
      "100%|██████████| 1306122/1306122 [02:18<00:00, 9401.22it/s]\n",
      "100%|██████████| 375806/375806 [00:30<00:00, 12164.93it/s]\n",
      "100%|██████████| 375806/375806 [00:38<00:00, 9769.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: preprocess(x))\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: tokenizer.tokenize(x, return_str=True))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: preprocess(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: tokenizer.tokenize(x, return_str=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2195fc5b36f080299d1d5497b1553fb7e33589eb"
   },
   "source": [
    "This is all roughly standard stuff. Only thing I think we could have improved here was a longer maxlen and larger vocabulary size, but we were having issues early on with running out of memory and killing kernels. We never really revisited this, but it seems everyone else was able to fit in many more words and multiple embeddings at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2c361bb3c71044528378aa551b0b7de76c45175d"
   },
   "outputs": [],
   "source": [
    "# some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters = '', lower = False)\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X) )\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6683124ae22ba417ffee16da8717ba9af5f8ae24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c901a544ae551841c07f5cd77ef8b6a6a8b0a70f"
   },
   "source": [
    "Using the gensim function to load in the embeddings ended up being much more time efficient than most of the things available in the public kernels. I am sure a few other people used this same function but I did not see it widespread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "daceae2e3c8387d7ec5185fc90fad23ecf04ebbf"
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def load_word2vec(fname, encoding='utf8', unicode_errors='strict',datatype=np.float32, max_vocab=3000000, word_index=None):\n",
    "    #emb_mean,emb_std = -0.0051106834, 0.18445626\n",
    "    #embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, 300))\n",
    "    embedding_index = {}\n",
    "    with utils.smart_open(fname) as fin:\n",
    "        header = utils.to_unicode(fin.readline(), encoding=encoding)\n",
    "        vocab_size, vector_size = (int(x) for x in header.split())\n",
    "        binary_len = np.dtype(datatype).itemsize * vector_size\n",
    "        \n",
    "        for _ in tqdm(range(min(vocab_size,max_vocab))):\n",
    "            # mixed text and binary: read text first, then binary\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = fin.read(1)\n",
    "                if ch == b' ':\n",
    "                    break\n",
    "                if ch == b'':\n",
    "                    raise EOFError(\"unexpected end of input\")\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            word = utils.to_unicode(b''.join(word), encoding=encoding, errors=unicode_errors)\n",
    "            weights = np.fromstring(fin.read(binary_len), dtype=datatype).astype(np.float16)\n",
    "            embedding_index[word] = weights\n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5939ead65b7fde889408741b8ee6f00dbb8f227b"
   },
   "source": [
    "Our functions for loading our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c38e4b4d8b51f3d10eeaaccb8f39696730223627"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_news(embed_dir = '../input/GoogleNews-vectors-negative300.bin'):\n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(embed_dir, binary=True)\n",
    "    emb_ind = {}\n",
    "    for i, vec in tqdm(enumerate(embeddings_index.wv.vectors)):\n",
    "        emb_ind[embeddings_index.wv.index2word[i]] = vec\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "    return emb_ind\n",
    "        \n",
    "def load_glove(word_index,embed_dir = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"):\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm_notebook(open(embed_dir, encoding = \"utf8\")) if o.split(\" \")[0] in word_index) \n",
    "    return embeddings_index\n",
    "\n",
    "def load_para(embed_dir ='../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'):\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm_notebook(open(embed_dir, encoding=\"utf8\", errors='ignore')) if len(o)>100)\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9724829776ed56541e61da2357f0ed2344925f5c"
   },
   "source": [
    "Our standard embedding matrix builder. We will revisit this in a second because we heavily altered things after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0a3b4d6db77fc946dddb12f6b03b1715576670ff"
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(word_index,embeddings_index, max_features, maxlen, lower = False):\n",
    "    #all_embs = np.stack(embeddings_index.values())\n",
    "    #emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    #embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, 300), dty)\n",
    "    embedding_matrix = np.zeros((max_features, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if lower:\n",
    "            word = word.lower()\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73cdd0816d6149c716b30ffa6976d9a4e522aa19"
   },
   "source": [
    "We only ended up using two of the below functions, add_lower_and_upper and add_symbols. What these do are add things to our embeddings which were missing. So if a word didnt show up in our embedding matrix, but we found that the lower case or uppercase was available in the pretrained embeddings then we would add that in where the blanks were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e758f8d5748115119e7e37d69711cc8daff9e732"
   },
   "outputs": [],
   "source": [
    "def add_lower_and_upper(embedding, vocab):\n",
    "    count_l = 0\n",
    "    count_u = 0\n",
    "    for word in tqdm(vocab):\n",
    "        if word not in embedding and word.lower() in embedding:  \n",
    "            embedding[word] = embedding[word.lower()]\n",
    "            count_l += 1\n",
    "        if word not in embedding and word.title() in embedding:\n",
    "            embedding[word] = embedding[word.title()]\n",
    "            count_u += 1\n",
    "    print(f\"Added {count_l} lower word embeddings\")\n",
    "    print(f\"Added {count_u} upper word embeddings\")\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def add_numbers(embedding, word_index):\n",
    "    count_d = 0\n",
    "    count_s = 0\n",
    "    for word in tqdm(word_index):\n",
    "        if word not in embedding:\n",
    "            cln = clean_numbers(word)\n",
    "            if cln in embedding:  \n",
    "                embedding[word] = embedding[cln]\n",
    "                count_d += 1\n",
    "    print(f\"Added {count_d} number embeddings\")\n",
    "    \n",
    "def add_symbols(embedding, word_index):\n",
    "    \n",
    "    quotes=[\"``\",\"''\"]\n",
    "    symbs_iv = [char for char in symbols if char in embedding]\n",
    "    mean_symbol = np.zeros((300,))\n",
    "    for s in symbs_iv:\n",
    "        mean_symbol += embedding[s]\n",
    "    \n",
    "    mean_symbol /= len(symbs_iv)\n",
    "\n",
    "    count_s = 0\n",
    "    for word in tqdm(word_index):\n",
    "        if word not in embedding:\n",
    "            if len(word) == 1:\n",
    "                embedding[word] = mean_symbol\n",
    "                count_s += 1\n",
    "            if word in quotes:\n",
    "                embedding[word] = mean_symbol\n",
    "                count_s += 1\n",
    "    print(f\"Added {count_s} symbol embeddings\")\n",
    "    \n",
    "def build_glove_matrix():\n",
    "    embeddings_index = load_glove(word_index=word_index,embed_dir = INPUT_PATH + \"embeddings/glove.840B.300d/glove.840B.300d.txt\")\n",
    "    add_lower_and_upper(embeddings_index, word_index)\n",
    "    add_symbols(embeddings_index, word_index)\n",
    "    embedding_matrix1 = build_embedding_matrix(word_index,embeddings_index, max_features, maxlen, lower = False)\n",
    "    return embedding_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "76ddfc8cb974535ec857e8109a3ad3fb0987b59c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4231ff849e74ef5847cc1ef0f6511c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 85738/280439 [00:00<00:00, 857376.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280439/280439 [00:00<00:00, 902734.20it/s]\n",
      " 41%|████      | 113646/280439 [00:00<00:00, 1136458.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1310 lower word embeddings\n",
      "Added 1922 upper word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280439/280439 [00:00<00:00, 1121553.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 351 symbol embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix1 = build_glove_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f0cf7382b448be61f9fccbb78f51b8aaa7d3c89d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1fc48f1f94d1fcd5a32e50dfb971148de2b9f593"
   },
   "outputs": [],
   "source": [
    "train_y = train_df[\"target\"].values\n",
    "if simulate_test: \n",
    "    test_y = test_df[\"target\"].values\n",
    "else:\n",
    "    test_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "cecf1b52afaf9f8e75e7eefd8a6db4557df8290b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, CuDNNGRU, CuDNNLSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Lambda, Permute, Reshape, merge, Dropout, Conv2D, MaxPool2D, Concatenate, Conv1D, MaxPool1D, add, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import glorot_uniform, he_uniform, he_normal\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU, multiply\n",
    "from keras.layers import Reshape, Permute, multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89334e7e31e6fce680686de49e8e9fcf41bbc1ed"
   },
   "source": [
    "Here is where our first major trick occurred. Full credit for Dieter for noticing this. What we did here was actually really clever to save time. After looking at the histograms of our predictions Dieter noticed that almost 70 percent of our predictions were under .01 meaning that the model was very confident that they were sincere questions. We also measured the accuracy of thes predictions and found that it was nearly perfect. Our models were able to find these easy ones within a few epochs. We exploited this by training one model in order to filter out the bottom 70 percent of the data and then retraining all of our models on the remaining 30%. This allowed us to train many more models in a quick span and more completely. After we train this model we simply throw it away so as not to leak any results into our validation set. All models after this one are trained from scratch only on the subset of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "47bf82b82bc4383477c1f1251e3f4331cf0ddf39"
   },
   "outputs": [],
   "source": [
    "def build_gru_model(embedding_matrix, name = \"gru\"):\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features,300,weights=[embedding_matrix],input_length=maxlen,trainable=False)(inp)\n",
    "    x = SpatialDropout1D(.4,seed=seed_nb)(x)\n",
    "    x = CuDNNGRU(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    x = CuDNNGRU(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool, last])\n",
    "    #x = Dropout(.3,seed=seed_nb)(x)\n",
    "    x = Dense(32, activation = 'relu',kernel_initializer=he_uniform(seed=seed_nb))(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=seed_nb))(x)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=outp, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "1db96986e11c89c87d17366d39012f18672a843f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1306122/1306122 [==============================] - 85s 65us/step - loss: 0.1314\n",
      "Epoch 2/3\n",
      "1306122/1306122 [==============================] - 84s 64us/step - loss: 0.1109\n",
      "Epoch 3/3\n",
      "1306122/1306122 [==============================] - 84s 64us/step - loss: 0.1059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f50fcc197f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "batch_size = 2048\n",
    "model = build_gru_model(embedding_matrix1, name = 'gru_stage0')\n",
    "model.compile(loss=binary_crossentropy,optimizer=Adam())\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=3,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "4819135e6603b13ce55d18e3141d959a0497c844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122/1306122 [==============================] - 28s 21us/step\n",
      "0.009684420283883804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, log_loss, f1_score\n",
    "\n",
    "\n",
    "filter_pred = model.predict(train_X, batch_size = batch_size, verbose =True)[:,0]\n",
    "thresh = np.percentile(filter_pred,70.)\n",
    "print(thresh)\n",
    "keep_index = (filter_pred > thresh)\n",
    "#trash_index = np.logical_not(keep_index)\n",
    "#trashed_pred = filter_pred[trash_index]\n",
    "#trashed_train = train_X[trash_index]\n",
    "#trashed_y = train_y[trash_index]\n",
    "\n",
    "#print('trashing data with these metrics:')\n",
    "#print('confusion matrix:')\n",
    "#print(confusion_matrix(trashed_pred>0.5,trashed_y))\n",
    "#print('acc:')\n",
    "#print(accuracy_score(trashed_pred>0.5,trashed_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f0c6821be2086718db47af401d0bdecd5291b896"
   },
   "outputs": [],
   "source": [
    "#k = np.where(keep_index)\n",
    "#t = np.where(np.logical_not(keep_index))\n",
    "#train_X_old = train_X.copy()\n",
    "#train_y_old = train_y.copy()\n",
    "train_X = train_X[np.where(keep_index)]\n",
    "train_y = train_y[np.where(keep_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ccd610b41dffdecbcab7cb9bb01451d204195a9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391837, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe2036b86fa5d8715154b9646e3879866fec4fab"
   },
   "source": [
    "Now you can see that our training set is much smaller and we only need to focus on the difficult problems. It is a waste of time in order to continue training on training samples the model is already confident and accurate on. These examples are trivial for one reason or another. We found that dropping these samples does not negatively affect our accuracy at all and our models trained on the subset can still easily classify those that were dropped and never trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "19451c0d9497e2318bc337897b3148a2a8354189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del filter_pred, model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43a451c0a6900f4dade3d56a6915b770bc495279"
   },
   "source": [
    "After filtering out these samples we used 5 models. These were found by training an array of models with different embeddings and configurations and then seeing how they ensembled together. Our offline stacker eventually found that the best ensemble of models was:\n",
    "* DPCNN with reversing and glove embeddings\n",
    "* A bidirectional gru into an lstm with the glove embeddings. (this was very similar to what we used for the toxic comment challenge and was our strongest individual model here as well)\n",
    "* a parrallel lstm and gru model w/glove embeddings\n",
    "* parts of speech bidirectional lstm and gru model w/paragram embeddings\n",
    "* parts of speech parallel lstm and gru model w/news embeddings\n",
    "\n",
    "I will talk about the POS models more in a second\n",
    "\n",
    "One thing I wish we had explored a bit more was stacked embeddings like many used. We simply were running into too many issues early on with killing kernels so I wrote it off as a viable option. In the toxic comment challenge we heavily utilized this with even stacking 3 embeddings at once and also encoding our own information in the embeddings like if the word was all_caps and other various features. We simply didnt have the throughput in order to use all of these techniques at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "e7a3ed60b660f8bd699cac03a1dede642cbcf5f7"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "2051f6715169fd62b1a8abd258b591f1bbd58a8d"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "627a9574c8db3aed8c31dbb9152354fbad082e2f"
   },
   "source": [
    "One major improvement we were able to add to all of our models because of this time savings was much higher spatial dropout and also a secondary training phase where we trained only the embedding layer. \n",
    "\n",
    "Spatial Dropout we found to be exceptionally effective to prevent overfitting in the toxic comment challenge. What this does is mask out dimensions of the embeddings. This is well described a couple places in the forums https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79911. The key is that your model is only seeing subsets of the dimensions of the embeddings at a time so it does not fit to all of them at once. Some people were doing this incorrectly in the pytorch variant where Dropout2D was dropping out entire words rather than just dimensions of the embeddings. We did some experiments of our own with PyTorch and confirmed this was not correct so @MihaS tried to correct this by manually creating a new dropout layer for us. We did not end up spending much more time with PyTorch though because the majority of us were proficient with Keras. \n",
    "\n",
    "The second portion, finetuning the embeddings is very crucial. Many people I saw used either unfrozen embeddings the entire time or left them frozen the entire training process. In my opinion this is not the correct way to approach it. If we look at how we do transfer learning with imagenet models and the like what people typically do is freeze the network except for the last layer and train that and then tune the rest of the network after that last layer is at least somewhat trained. The reason behind doing this is if you train the entire network and your last layer is randomly initialized then it will essentially just blow out whatever was there because the gradient will be so noisy. If you do that you are using them as initialization points rather than to actually transfer knowledge to a new problem. The same is true with word embeddings. We train the rest of the network around the embeddings and then once that is adequately trained we can do some tweaking of the embeddings. In this way we are able to maintain all of the juicy information captured in these embeddings and also impart some of the information from this new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "6eca7e28ba773553f53a44b6c2d3388ded4d7a54"
   },
   "outputs": [],
   "source": [
    "def train_model(model,train_x, y_tra,epochs,batch_size):\n",
    "    \n",
    "    \n",
    "    #complete_hist = {'model_name':model.name}\n",
    "    #complete_hist['epochs'] = [0,0,0]\n",
    "    #tic = time.time()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['acc', f1])\n",
    "\n",
    "    hist = model.fit(train_x, y_tra, batch_size=batch_size, epochs=epochs[1],verbose=True)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[1].trainable = True\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['acc', f1])\n",
    "    hist = model.fit(train_x, y_tra, batch_size=batch_size, epochs=epochs[2],verbose=True)\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "9d74f9700d282d37f11d26d5714f8615ca50a6bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "ebef366dc1ce73699fb3dda7502ecffd480b3981"
   },
   "outputs": [],
   "source": [
    "def build_dpcnn_model(embedding_matrix, num_block = 5, k = 3, units = 64,name = 'dpcnn'):\n",
    "    pad = 'same'\n",
    "    inp = Input(shape = (None, ))\n",
    "    embedding_layer1 = Embedding(max_features, embed_size, weights=[embedding_matrix], embeddings_initializer = he_uniform(seed=seed_nb), trainable = False)(inp)\n",
    "    embedding_layer1 = SpatialDropout1D(0.3)(embedding_layer1)\n",
    "    emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(embedding_layer1)\n",
    "    emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(emb_short_cut)\n",
    "    # Main block\n",
    "    for b in range(1, num_block + 1):\n",
    "        if b == 1:\n",
    "            block = embedding_layer1\n",
    "            short_cut = emb_short_cut\n",
    "        else:\n",
    "            block = block\n",
    "            short_cut = block\n",
    "        block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
    "        block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
    "        block = add([short_cut, block])\n",
    "        block = MaxPooling1D(pool_size = 3, strides = 2, padding = pad)(block)\n",
    "    # Final block\n",
    "    short_cut = block\n",
    "    block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
    "    block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
    "    block = add([short_cut, block])\n",
    "    max_pool = GlobalMaxPooling1D()(block)\n",
    "    avg_pool = GlobalAveragePooling1D()(block)\n",
    "    last = (Lambda(lambda x: x[:,-1,:]) (block))\n",
    "    block = concatenate([max_pool, avg_pool, last])\n",
    "    #reverse\n",
    "    rev_embedding_layer = Lambda(lambda x: K.reverse(x,axes=-1))(embedding_layer1)\n",
    "    rev_emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(rev_embedding_layer)\n",
    "    rev_emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(rev_emb_short_cut)\n",
    "    # Main block\n",
    "    for b in range(1, num_block + 1):\n",
    "        if b == 1:\n",
    "            rev_block = rev_embedding_layer\n",
    "            rev_short_cut = rev_emb_short_cut\n",
    "        else:\n",
    "            rev_block = rev_block\n",
    "            rev_short_cut = rev_block\n",
    "        rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
    "        rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
    "        rev_block = add([rev_short_cut, rev_block])\n",
    "        rev_block = MaxPooling1D(pool_size = 3, strides = 2, padding = pad)(rev_block)\n",
    "    # Final block\n",
    "    rev_short_cut = rev_block\n",
    "    rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
    "    rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
    "    rev_block = add([rev_short_cut, rev_block])\n",
    "    rev_max_pool = GlobalMaxPooling1D()(rev_block)\n",
    "    rev_avg_pool = GlobalAveragePooling1D()(rev_block)\n",
    "    rev_last = (Lambda(lambda x: x[:,-1,:]) (rev_block))\n",
    "    rev_block = concatenate([rev_max_pool, rev_avg_pool, rev_last])\n",
    "    block = concatenate([rev_block, block])\n",
    "    # output block\n",
    "    out_put = Dense(64, activation = 'relu')(block)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(out_put)\n",
    "    model = Model(inputs=inp, outputs=outp, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "521bcee948279e41dcb473a8499ac31b76aabdc5"
   },
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "preds_test_all = np.zeros((test_X.shape[0],n_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "68f8de54adcd1e005938f248709f8b1fe5559367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "391837/391837 [==============================] - 55s 141us/step - loss: 0.3880 - acc: 0.8391 - f1: 0.4805\n",
      "Epoch 2/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.3287 - acc: 0.8613 - f1: 0.6117\n",
      "Epoch 3/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.3147 - acc: 0.8663 - f1: 0.6334\n",
      "Epoch 4/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.3057 - acc: 0.8701 - f1: 0.6470\n",
      "Epoch 5/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.2983 - acc: 0.8732 - f1: 0.6589\n",
      "Epoch 6/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.2926 - acc: 0.8758 - f1: 0.6681\n",
      "Epoch 7/12\n",
      "391837/391837 [==============================] - 51s 131us/step - loss: 0.2874 - acc: 0.8777 - f1: 0.6748\n",
      "Epoch 8/12\n",
      "391837/391837 [==============================] - 51s 130us/step - loss: 0.2831 - acc: 0.8796 - f1: 0.6810\n",
      "Epoch 9/12\n",
      "391837/391837 [==============================] - 51s 130us/step - loss: 0.2798 - acc: 0.8809 - f1: 0.6851\n",
      "Epoch 10/12\n",
      "391837/391837 [==============================] - 51s 130us/step - loss: 0.2755 - acc: 0.8830 - f1: 0.6924\n",
      "Epoch 11/12\n",
      "391837/391837 [==============================] - 51s 130us/step - loss: 0.2718 - acc: 0.8843 - f1: 0.6969\n",
      "Epoch 12/12\n",
      "391837/391837 [==============================] - 51s 130us/step - loss: 0.2687 - acc: 0.8854 - f1: 0.7007\n",
      "Epoch 1/3\n",
      "391837/391837 [==============================] - 61s 155us/step - loss: 0.2570 - acc: 0.8907 - f1: 0.7155\n",
      "Epoch 2/3\n",
      "391837/391837 [==============================] - 58s 149us/step - loss: 0.2408 - acc: 0.8978 - f1: 0.7352\n",
      "Epoch 3/3\n",
      "391837/391837 [==============================] - 58s 149us/step - loss: 0.2262 - acc: 0.9046 - f1: 0.7544\n",
      "375806/375806 [==============================] - 17s 44us/step\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_name = 'dpcnn'\n",
    "batch_size = 1000\n",
    "epochs = [0,12,3] # checked locally\n",
    "\n",
    "model = build_dpcnn_model(embedding_matrix1)\n",
    "dpcnn_hist = train_model(model,train_X, train_y, epochs,batch_size)\n",
    "preds_test_all[:,0] = model.predict(test_X, batch_size=batch_size,verbose = True)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "aa7d5336a15af5bd18eb59a7c32e626c9f493a79"
   },
   "outputs": [],
   "source": [
    "\n",
    "#ens_pred1 = model.predict(X_ens, batch_size=batch_size,verbose = True)\n",
    "#os.remove(\"dpcnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "82513b59008ff5605f24b8aa0b9907e17884a27d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model, dpcnn_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "79e4ee4e5ceabcb2f6c7762f433e0af9abffe238"
   },
   "outputs": [],
   "source": [
    "def build_gru_lstm_model(embedding_matrix, name = \"\"):\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(.4, seed = seed_nb)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb)))(x)\n",
    "    x = Dropout(.2, seed = seed_nb)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb)))(x)\n",
    "\n",
    "    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool,max_pool,last])\n",
    "    x = Dropout(.3, seed = seed_nb)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=seed_nb))(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=outp, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "5da9751999b71ebfbbdbd32f85c8981f3e9f1f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "391837/391837 [==============================] - 65s 166us/step - loss: 0.4008 - acc: 0.8284 - f1: 0.4102\n",
      "Epoch 2/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3472 - acc: 0.8528 - f1: 0.5732\n",
      "Epoch 3/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3303 - acc: 0.8609 - f1: 0.6097\n",
      "Epoch 4/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3202 - acc: 0.8652 - f1: 0.6277\n",
      "Epoch 5/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3130 - acc: 0.8683 - f1: 0.6403\n",
      "Epoch 6/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3075 - acc: 0.8701 - f1: 0.6466\n",
      "Epoch 7/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.3010 - acc: 0.8733 - f1: 0.6580\n",
      "Epoch 8/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.2975 - acc: 0.8745 - f1: 0.6627\n",
      "Epoch 9/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.2926 - acc: 0.8760 - f1: 0.6680\n",
      "Epoch 10/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.2878 - acc: 0.8780 - f1: 0.6751\n",
      "Epoch 11/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.2856 - acc: 0.8790 - f1: 0.6783\n",
      "Epoch 12/12\n",
      "391837/391837 [==============================] - 64s 163us/step - loss: 0.2810 - acc: 0.8804 - f1: 0.6830\n",
      "Epoch 1/3\n",
      "391837/391837 [==============================] - 75s 193us/step - loss: 0.2728 - acc: 0.8848 - f1: 0.6965\n",
      "Epoch 2/3\n",
      "391837/391837 [==============================] - 74s 188us/step - loss: 0.2609 - acc: 0.8897 - f1: 0.7120\n",
      "Epoch 3/3\n",
      "391837/391837 [==============================] - 74s 188us/step - loss: 0.2509 - acc: 0.8943 - f1: 0.7257\n",
      "375806/375806 [==============================] - 20s 53us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "314083"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_name = 'gru_lstm'\n",
    "batch_size = 2048\n",
    "epochs = [0,12,3] # checked locally\n",
    "\n",
    "model = build_gru_lstm_model(embedding_matrix1)\n",
    "lstm_gru_hist = train_model(model,train_X, train_y, epochs,batch_size)\n",
    "preds_test_all[:,1] = model.predict(test_X, batch_size=batch_size,verbose = True)[:,0]\n",
    "del model, lstm_gru_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "437dbd18eb8a3873f044865c6f901c87b15621df"
   },
   "outputs": [],
   "source": [
    "def build_parallel_lstm_gru(embedding_matrix, name = \"\"):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(.4, seed = seed_nb)(x)\n",
    "    x1 = CuDNNGRU(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    x2 = CuDNNLSTM(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    x = Dropout(.2, seed = seed_nb)(x)\n",
    "    x1 = CuDNNGRU(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    x2 = CuDNNLSTM(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb))(x)\n",
    "    x = Concatenate()([x1,x2])\n",
    "    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool,  last])\n",
    "    #x = Dropout(.3, seed = seed_nb)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=seed_nb))(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=outp, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "2858093c20852a6692edfc1ec15ffc4846f7d2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "391837/391837 [==============================] - 63s 160us/step - loss: 0.3954 - acc: 0.8312 - f1: 0.4348\n",
      "Epoch 2/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3464 - acc: 0.8534 - f1: 0.5756\n",
      "Epoch 3/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3308 - acc: 0.8606 - f1: 0.6067\n",
      "Epoch 4/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3195 - acc: 0.8655 - f1: 0.6273\n",
      "Epoch 5/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3126 - acc: 0.8684 - f1: 0.6388\n",
      "Epoch 6/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3062 - acc: 0.8706 - f1: 0.6483\n",
      "Epoch 7/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3010 - acc: 0.8731 - f1: 0.6570\n",
      "Epoch 8/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2970 - acc: 0.8742 - f1: 0.6615\n",
      "Epoch 9/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2921 - acc: 0.8761 - f1: 0.6687\n",
      "Epoch 10/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2878 - acc: 0.8781 - f1: 0.6750\n",
      "Epoch 11/11\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2848 - acc: 0.8791 - f1: 0.6785\n",
      "Epoch 1/4\n",
      "391837/391837 [==============================] - 73s 186us/step - loss: 0.2755 - acc: 0.8834 - f1: 0.6919\n",
      "Epoch 2/4\n",
      "391837/391837 [==============================] - 71s 181us/step - loss: 0.2639 - acc: 0.8880 - f1: 0.7057\n",
      "Epoch 3/4\n",
      "391837/391837 [==============================] - 71s 181us/step - loss: 0.2541 - acc: 0.8924 - f1: 0.7193\n",
      "Epoch 4/4\n",
      "391837/391837 [==============================] - 71s 181us/step - loss: 0.2456 - acc: 0.8963 - f1: 0.7309\n",
      "375806/375806 [==============================] - 18s 48us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_name = 'parallel_lstm_gru2'\n",
    "batch_size = 2048\n",
    "epochs = [0,11,4] # checked locally\n",
    "\n",
    "model = build_parallel_lstm_gru(embedding_matrix1)\n",
    "parallel_lstm_gru2_hist = train_model(model,train_X, train_y, epochs,batch_size)\n",
    "preds_test_all[:,2] = model.predict(test_X, batch_size=batch_size,verbose = True)[:,0]\n",
    "del model, parallel_lstm_gru2_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5374ae7bb08148fe00f1d4580eea21df3f1f7311"
   },
   "source": [
    "The below section is where we did POS tagging in order to disambiguate the words. I have a more detailed write up of this in my other public kernel https://www.kaggle.com/ryches/parts-of-speech-disambiguation-error-analysis. This technique on its own performs worse (I have some hypotheses of why this might be the case. It has to do with the way I am initializing them in the same spot. If I were to do something to break their symmetry I think it would likely perform even better) but when ensembled, since the model is learning based off of slightly more expressive embeddings, it actually performs significantly better. The offline stacker gave these much higher weights than initially expected. I used this same technique in toxic comment, but I was not able to extensively test how it worked and my method is much more elegant now. Last competition I simply sent the results to alex and he gave me the thumbs up saying it seemed to get weight in the stacker because we found out so late in the competition. This time I got to actually play with it and see it for myself. This took our 5 model cv from .702 to upper .705. when stacked. We were only able to do this by training on the subset. With a full training set we would have run out of time. Especially with the expanded test set. \n",
    "\n",
    "Our other submission was able to fit 8 models in one kernel, but it turns out it was worth dropping those models in place of the POS tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa95ae17536308295381c13bc60729b5e764c385"
   },
   "source": [
    "There are various difficulties with the POS tagging, specifically making the text actually line up with the parts of speech that are output. Keras tokenizer will typically break these so I had to take care with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "30555ee5044cf2e48969a476c0c4d73a44e067f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "391837it [05:54, 1105.96it/s]\n",
      "375806it [04:56, 1268.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "train_X = train_X[np.where(keep_index)]\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "    def __call__(self, text):\n",
    "        words = text.strip().split(' ')\n",
    "        words = [word for word in words if word is not '']\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "nlp = spacy.load('en', disable = [\"parser\", \"ner\", \"textcat\"])\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "tokens = []\n",
    "for doc in tqdm(nlp.pipe(train_X, n_threads = 8)):\n",
    "    tokens.append(\" \".join([n.text + \"_\"  + n.pos_ for n in doc]))\n",
    "train_X = tokens\n",
    "tokens = []\n",
    "for doc in tqdm(nlp.pipe(test_X, n_threads = 8)):\n",
    "    tokens.append(\" \".join([n.text + \"_\"  + n.pos_ for n in doc]))\n",
    "test_X = tokens\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters = '', lower = False)\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X) )\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "46e5011e79e9e52b1c8e7d2e406338160d7841db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be8f0265ca64e8f82d60b25013ce951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 107854/207626 [00:00<00:00, 552595.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207626/207626 [00:00<00:00, 499680.15it/s]\n",
      "100%|██████████| 207626/207626 [00:00<00:00, 1107349.93it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 63871 lower word embeddings\n",
      "Added 3 upper word embeddings\n",
      "Added 0 symbol embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_embedding_matrix(word_index,embeddings_index, max_features, maxlen, lower = False):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, 300))\n",
    "    embedding_matrix = np.zeros((max_features, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        word_part = word.split(\"_\")[0]\n",
    "        embedding_vector = embeddings_index.get(word_part)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "def add_lower_and_upper(embedding, vocab):\n",
    "    count_l = 0\n",
    "    count_u = 0\n",
    "    for word in tqdm(vocab):\n",
    "        word = word.split(\"_\")[0]\n",
    "        if word not in embedding and word.lower() in embedding:  \n",
    "            embedding[word] = embedding[word.lower()]\n",
    "            count_l += 1\n",
    "        if word not in embedding and word.title() in embedding:\n",
    "            embedding[word] = embedding[word.title()]\n",
    "            count_u += 1\n",
    "    print(f\"Added {count_l} lower word embeddings\")\n",
    "    print(f\"Added {count_u} upper word embeddings\")\n",
    "embeddings_index2 = load_para(embed_dir = INPUT_PATH + 'embeddings/paragram_300_sl999/paragram_300_sl999.txt')\n",
    "add_lower_and_upper(embeddings_index2, word_index)\n",
    "add_symbols(embeddings_index2, word_index)\n",
    "embedding_matrix1 = build_embedding_matrix(word_index,embeddings_index2, max_features, maxlen, lower = True)\n",
    "del embeddings_index2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "64e942f57e191a305719895e858f9b5b7eece003"
   },
   "outputs": [],
   "source": [
    "def build_lstm_gru_model(embedding_matrix, name = \"\"):\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(.4, seed = seed_nb)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb)))(x)\n",
    "    x = Dropout(.2, seed = seed_nb)(x)\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True,kernel_initializer=glorot_uniform(seed=seed_nb)))(x)\n",
    "\n",
    "    last = Flatten()(Lambda(lambda x: x[:,-1:,:]) (x))\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool,max_pool,last])\n",
    "    x = Dropout(.3, seed = seed_nb)(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=seed_nb))(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=outp, name = name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "19bd8c92cfa5bdc5441c6d31e02c6eba5b03e4c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "391837/391837 [==============================] - 68s 174us/step - loss: 0.3993 - acc: 0.8315 - f1: 0.4152\n",
      "Epoch 2/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3478 - acc: 0.8527 - f1: 0.5678\n",
      "Epoch 3/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3317 - acc: 0.8598 - f1: 0.6019\n",
      "Epoch 4/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3217 - acc: 0.8643 - f1: 0.6207\n",
      "Epoch 5/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3148 - acc: 0.8667 - f1: 0.6320\n",
      "Epoch 6/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3080 - acc: 0.8701 - f1: 0.6436\n",
      "Epoch 7/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.3020 - acc: 0.8727 - f1: 0.6540\n",
      "Epoch 8/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.2967 - acc: 0.8749 - f1: 0.6620\n",
      "Epoch 9/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.2922 - acc: 0.8770 - f1: 0.6694\n",
      "Epoch 10/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.2885 - acc: 0.8792 - f1: 0.6762\n",
      "Epoch 11/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.2838 - acc: 0.8803 - f1: 0.6802\n",
      "Epoch 12/13\n",
      "391837/391837 [==============================] - 67s 172us/step - loss: 0.2800 - acc: 0.8822 - f1: 0.6867\n",
      "Epoch 13/13\n",
      "391837/391837 [==============================] - 67s 171us/step - loss: 0.2766 - acc: 0.8833 - f1: 0.6910\n",
      "Epoch 1/3\n",
      "391837/391837 [==============================] - 78s 199us/step - loss: 0.2677 - acc: 0.8874 - f1: 0.7028\n",
      "Epoch 2/3\n",
      "391837/391837 [==============================] - 77s 196us/step - loss: 0.2587 - acc: 0.8906 - f1: 0.7128\n",
      "Epoch 3/3\n",
      "391837/391837 [==============================] - 77s 196us/step - loss: 0.2506 - acc: 0.8949 - f1: 0.7256\n",
      "375806/375806 [==============================] - 22s 58us/step\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_name = 'lstm_gru'\n",
    "batch_size = 2048\n",
    "epochs = [0,13,3] # checked locally\n",
    "\n",
    "model = build_lstm_gru_model(embedding_matrix1)\n",
    "lstm_gru_hist = train_model(model,train_X, train_y, epochs,batch_size)\n",
    "preds_test_all[:,3] = model.predict(test_X, batch_size=batch_size,verbose = True)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "98a758069947526feb16202410e82b0228a9bc78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model, lstm_gru_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "1c2d8fd9d3b23ac359f537108be3130d390de14b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding_matrix1\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "d8bd4c560eb044df347003617314c00936f4a76b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000000 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "100%|██████████| 3000000/3000000 [00:42<00:00, 70032.90it/s]\n",
      "100%|██████████| 207626/207626 [00:00<00:00, 611030.78it/s]\n",
      "  7%|▋         | 14745/207626 [00:00<00:01, 147448.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1516 lower word embeddings\n",
      "Added 6329 upper word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207626/207626 [00:01<00:00, 154646.24it/s]\n",
      " 46%|████▌     | 95030/207626 [00:00<00:00, 950192.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 number embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207626/207626 [00:00<00:00, 982454.36it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 symbol embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:107: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index3 = load_word2vec(INPUT_PATH + 'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin')\n",
    "add_lower_and_upper(embeddings_index3, word_index)\n",
    "add_numbers(embeddings_index3, word_index)\n",
    "add_symbols(embeddings_index3, word_index)\n",
    "embedding_matrix1 = build_embedding_matrix(word_index,embeddings_index3, max_features, maxlen, lower = False)\n",
    "del embeddings_index3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "ba3441f9d8f9c7dfc1db092d4465e0b7884e11b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "391837/391837 [==============================] - 63s 160us/step - loss: 0.4080 - acc: 0.8261 - f1: 0.3914\n",
      "Epoch 2/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3604 - acc: 0.8468 - f1: 0.5440\n",
      "Epoch 3/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3438 - acc: 0.8549 - f1: 0.5819\n",
      "Epoch 4/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3345 - acc: 0.8591 - f1: 0.6016\n",
      "Epoch 5/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3280 - acc: 0.8619 - f1: 0.6135\n",
      "Epoch 6/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3216 - acc: 0.8645 - f1: 0.6243\n",
      "Epoch 7/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3169 - acc: 0.8658 - f1: 0.6302\n",
      "Epoch 8/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3128 - acc: 0.8684 - f1: 0.6396\n",
      "Epoch 9/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3081 - acc: 0.8697 - f1: 0.6450\n",
      "Epoch 10/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3053 - acc: 0.8710 - f1: 0.6499\n",
      "Epoch 11/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.3011 - acc: 0.8726 - f1: 0.6561\n",
      "Epoch 12/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2986 - acc: 0.8734 - f1: 0.6584\n",
      "Epoch 13/13\n",
      "391837/391837 [==============================] - 62s 157us/step - loss: 0.2951 - acc: 0.8756 - f1: 0.6661\n",
      "Epoch 1/3\n",
      "391837/391837 [==============================] - 72s 184us/step - loss: 0.2854 - acc: 0.8795 - f1: 0.6804\n",
      "Epoch 2/3\n",
      "391837/391837 [==============================] - 71s 181us/step - loss: 0.2617 - acc: 0.8893 - f1: 0.7105\n",
      "Epoch 3/3\n",
      "391837/391837 [==============================] - 71s 181us/step - loss: 0.2431 - acc: 0.8980 - f1: 0.7360\n",
      "375806/375806 [==============================] - 18s 48us/step\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model_name = 'parallel_lstm_gru3'\n",
    "batch_size = 2048\n",
    "epochs = [0,13,3] # checked locally\n",
    "\n",
    "model = build_parallel_lstm_gru(embedding_matrix1)\n",
    "parallel_lstm_gru3_hist = train_model(model,train_X, train_y, epochs,batch_size)\n",
    "preds_test_all[:,4] = model.predict(test_X, batch_size=batch_size,verbose = True)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "c16bd80147e6a43c1490e0331a8995d447404e88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model, parallel_lstm_gru3_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "ae0d6983937b4dcaf89cb10ec90ffd1f44c9c39e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedding_matrix1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88ead96e6c3a76d1718958c55a2bf26ec7bd6be0"
   },
   "source": [
    "These are the weights that were found offline using hillclimbing. These are not really all that precise. At the end we ran out of time so I simply ran things once and this is what was popped out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "152837f3c34ce1ab4811e5badf3ad72c71ef606c"
   },
   "outputs": [],
   "source": [
    "w = np.array([41, 95, 66, 68, 87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "481e3a963a572523860bab152199643114c18847"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06502898592737692"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.zeros((test_X.shape[0]))\n",
    "for i in range(5):\n",
    "    preds += w[i]*preds_test_all[:,i]\n",
    "preds/= np.sum(w)\n",
    "preds.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16fba592031ac6be62f8323d7175732dbf276835"
   },
   "source": [
    "Our threshold was set to .34 because this seemed to work reasonably well in our CV. One thing to note is we totally dropped any sort of validation come rollout time in order to maximize the training data our model was exposed to. This ended up having a significant effect on our performance. \n",
    "\n",
    "The threshold likely could have been more precisely selected. \n",
    "\n",
    "It's unfortunate we weren't able to further exploit the tricks we came across, but glad the decision to give a shot to the time consuming POS tagging step paid off. Once again it shows how important preprocessing is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "a47507e809ff5df6917191e1522d30d083448a63"
   },
   "outputs": [],
   "source": [
    "y_te = np.array(preds>0.34, dtype=np.int8)\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "c326301fcb3c89e1a0ef7f419c23faba197a2ebc"
   },
   "outputs": [],
   "source": [
    "# submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": pred})\n",
    "# submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "6bbdd782c2625373c91cb40678a835fad0da932a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
