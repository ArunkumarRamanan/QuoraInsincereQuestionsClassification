{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GroupShuffleSplit\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "78578eab64a477d0a5ad6b1c917ae154868a44df"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\",usecols=['question_text','target'])\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.001, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3eaf3fc1e7d31b41a4a94523cf8b2fb9c526042b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1224076\n",
       "1    80739  \n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cac5122ea89be2d501e0e2cf5f1b62e99e3e86d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.945677\n",
       "1    0.054323\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7a16003f33ff26675d1a9c5f4feafc5357c7b087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 204 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regex = re.compile('[^a-zA-Z0-9\\$]')\n",
    "def clear_digit(m):\n",
    "    return m.group(1) + m.group(3)\n",
    "def clear_dot_name(m):\n",
    "    return m.group(1) + m.group(3) + \" \"\n",
    "def sep_digit_leter(m):\n",
    "    return m.group(1) + \" \" + m.group(2)\n",
    "\n",
    "\n",
    "def preprocessing(X):\n",
    "#First parameter is the replacement, second parameter is your input string\n",
    "    #\n",
    "    X = [x.replace('â€™', '\\'') for x in X]\n",
    "    X = [x.replace('$', ' $ ') for x in X]\n",
    "    \n",
    "    X = [x.replace('\\'s ',' sssssssss ') for x in X]\n",
    "    \n",
    "    X = [x.replace('i\\'m ','i am ') for x in X]\n",
    "    X = [x.replace('I\\'m ','I am ') for x in X]\n",
    "    \n",
    "    \n",
    "    X = [x.replace('\\'re ',' are ') for x in X]\n",
    "    X = [x.replace('\\'ve ',' have ') for x in X]  \n",
    "    X = [x.replace('won\\'t ','will not ') for x in X]\n",
    "    X = [x.replace('n\\'t ',' not ') for x in X]\n",
    "    X = [x.replace('\\'ll ',' will ') for x in X]\n",
    "    X = [x.replace('\\'d ',' ddddddddd ') for x in X]\n",
    "    #X = [x.replace('U.S.', ' USA ') for x in X]\n",
    "    \n",
    "    #X = [x.replace('B.S.', ' BS ') for x in X]\n",
    "    #X = [x.replace('M.S.', ' MS ') for x in X]\n",
    "    X = [x.replace('e.g.', ' ') for x in X]\n",
    "    #X = [x.lower() for x in X]\n",
    "    \n",
    "    \n",
    "    X = [re.sub('\\[math\\].*?math\\]', ' equation ', x) for x in X]\n",
    "    #X = [re.sub('\\(.*?\\)', ' ', x) for x in X]\n",
    "    X = [x.replace('B.Tech',  ' BS ') for x in X]\n",
    "    X = [x.replace('M.Tech',  ' MS ') for x in X]\n",
    "    X = [x.replace('Mr. ',  ' Mr ') for x in X]\n",
    "    X = [x.replace('Mrs. ',  ' Mrs ') for x in X]\n",
    "    X = [x.replace('Ms. ',  ' Ms ') for x in X]\n",
    "    X = [re.sub(\"(http|Http|www\\.).*?( |$)\", ' link ', x) for x in X]\n",
    "    X = [re.sub(\"([0-9])(,)([0-9])\", clear_digit, x) for x in X]\n",
    "    X = [re.sub(\"([0-9])([a-z])\", sep_digit_leter, x) for x in X]\n",
    "    X = [re.sub(\"([a-z])([0-9])\", sep_digit_leter, x) for x in X]\n",
    "    X = [re.sub(\"([A-Z])(\\.)([A-Z]{0,1})([a-z]{0,1})(\\.{0,1})\", clear_digit, x) for x in X]\n",
    "    \n",
    "    \n",
    "    X = [re.sub('\\.+',' aaaaaaaaa ',x) for x in X]\n",
    "    X = [re.sub(',+',' bbbbbbbbb ',x) for x in X]\n",
    "    X = [x.replace('?',  ' ccccccccc ') for x in X]\n",
    "    X = [x.replace('!',  ' vvvvvvvvv ') for x in X]\n",
    "    \n",
    "    \n",
    "    X = [regex.sub(' ', x) for x in X]\n",
    "    \n",
    "    X = [x.replace(' US ', ' USA ') for x in X]\n",
    "    \n",
    "    X = [x.lower() for x in X]\n",
    "    for i in '0123456789':\n",
    "        X = [x.replace(i, '#') for x in X]\n",
    "    \n",
    "    X = [x.replace(' aaaaaaaaa ', ' . ') for x in X]\n",
    "    X = [x.replace(' bbbbbbbbb ',' , ') for x in X]\n",
    "    X = [x.replace(' ccccccccc ',' ? ') for x in X]\n",
    "    X = [x.replace(' vvvvvvvvv ',' ! ') for x in X]\n",
    "    X = [x.replace(' sssssssss ',' \\'s ') for x in X]\n",
    "    X = [x.replace(' ddddddddd ',' \\'d ') for x in X]\n",
    "    \n",
    "    X = [x.replace('quorans', 'quora') for x in X]\n",
    "    X = [x.replace('quoran', 'quora') for x in X]\n",
    "    X = [x.replace('qoura', 'quora') for x in X]\n",
    "    X = [x.replace('cryptocurrencies', 'bitcoin') for x in X]\n",
    "    X = [x.replace('redmi', 'phone') for x in X]\n",
    "    X = [x.replace('oneplus', 'phone') for x in X]\n",
    "    X = [x.replace('lenovo','laptop') for x in X]\n",
    "    return X\n",
    "\n",
    "#train_df['question_text'].iloc[37859:37860] ['question_text'] = \"What are Loy Machedo's thoughts on evil spirit?\"\n",
    "\n",
    "train_df['transform'] = preprocessing(train_df['question_text'])\n",
    "val_df['transform'] = preprocessing(val_df['question_text'])\n",
    "test_df['transform'] = preprocessing(test_df['question_text'])\n",
    "\n",
    "#Out: 'abdE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e23864b5af79a943d5d081093a0da6d23e9e1f15"
   },
   "outputs": [],
   "source": [
    "#train_df['transform'] = train_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split() ]).lower() )\n",
    "#test_df['transform'] = test_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split()]).lower() )\n",
    "#val_df['transform'] = val_df['transform'].apply(lambda x: \" \".join([i if (len(i) <= 2 or i.upper() != i) else \"something\" for i in x.split() ]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1b326f67d6f1ab028c37699cbb2acccc5b88dd53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>885050</th>\n",
       "      <td>What are the limiting factors of a rainforest?</td>\n",
       "      <td>0</td>\n",
       "      <td>what are the limiting factors of a rainforest ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152979</th>\n",
       "      <td>Have you ever been late to an interview and still got the job?</td>\n",
       "      <td>0</td>\n",
       "      <td>have you ever been late to an interview and still got the job ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570013</th>\n",
       "      <td>Why do Quorans like to talk about their personal life here?</td>\n",
       "      <td>0</td>\n",
       "      <td>why do quora like to talk about their personal life here ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104010</th>\n",
       "      <td>What are some of the best biographical articles about Lionel Rose?</td>\n",
       "      <td>0</td>\n",
       "      <td>what are some of the best biographical articles about lionel rose ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352946</th>\n",
       "      <td>What should I do to those who used me and dumped me?</td>\n",
       "      <td>0</td>\n",
       "      <td>what should i do to those who used me and dumped me ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             question_text                                  ...                                                                                              transform\n",
       "885050  What are the limiting factors of a rainforest?                                                      ...                                   what are the limiting factors of a rainforest ?                     \n",
       "152979  Have you ever been late to an interview and still got the job?                                      ...                                   have you ever been late to an interview and still got the job ?     \n",
       "570013  Why do Quorans like to talk about their personal life here?                                         ...                                   why do quora like to talk about their personal life here ?          \n",
       "104010  What are some of the best biographical articles about Lionel Rose?                                  ...                                   what are some of the best biographical articles about lionel rose ? \n",
       "352946  What should I do to those who used me and dumped me?                                                ...                                   what should i do to those who used me and dumped me ?               \n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "86e48a32417b8042266b07d8f7f7ca29f185648a"
   },
   "outputs": [],
   "source": [
    "X_train = train_df['transform'].values\n",
    "X_val = val_df['transform'].values\n",
    "y_train = train_df['target'].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df['transform'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6dbc9be6f1ed3e522118b9cb0c3d0820484d8b0e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "deb61e6af84236669980e7b31b4258166f33129f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.5 s, sys: 292 ms, total: 49.8 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxlen = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "# tokenizer = text.Tokenizer(filters='\\t\\n',oov_token=set(list(oov[0].values)))\n",
    "# tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post',truncating='post')\n",
    "x_val = sequence.pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "07b77bbf3fc8b5b18462a05d476e369f3a9fb432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180634"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix1 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\n",
    "embedding_matrix2 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\n",
    "#embedding_matrix3 = np.zeros((max(list(word_index.values())) + 1, 300), dtype = 'float32')\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "92ffbf2ef35d2dc5863ee27c61eadd1869ca5440"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:53, 41261.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# embdedding setup\n",
    "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#embeddings_index = {}\n",
    "f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    if (len(values) < 200):\n",
    "        print(\"a\")\n",
    "    word = values[0]\n",
    "    if word not in  word_index:\n",
    "        continue\n",
    "    embedding_matrix1[word_index[word]] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "026d2cc756fc2fb517c7e6978cd90e8b2706b4b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ed767fad81efcbdbae23e0a5ca0e095f8de5eefa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1703756it [00:45, 37093.52it/s]\n"
     ]
    }
   ],
   "source": [
    "f = open('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt', encoding=\"utf8\", errors='ignore')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    if word in  word_index:\n",
    "        embedding_matrix2[word_index[word]] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e56b4ce49b4743372ef2c75703225582a2b9d81d"
   },
   "outputs": [],
   "source": [
    "#tmp = embedding_matrix1.sum(axis=1)\n",
    "tmp = pd.DataFrame(list(tokenizer.word_index.items()))\n",
    "tmp[2] = (embedding_matrix1.sum(axis=1)==0)[1:]\n",
    "tmp[3] = (embedding_matrix2.sum(axis=1)==0)[1:]\n",
    "#tmp[4] = (embedding_matrix3.sum(axis=1)==0)[1:]\n",
    "#a = tmp[tmp[2]][0][:1000].values\n",
    "#a1 = tmp[tmp[2]][0][:1000].index\n",
    "\n",
    "tmp = tmp[tmp[2] | tmp[3]][:4000]\n",
    "name = tmp[0].values\n",
    "indexes = tmp[1].values\n",
    "embedding_matrix1[indexes] = 0\n",
    "embedding_matrix2[indexes] = 0\n",
    "#embedding_matrix3[indexes] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "90f0fe5389b709cebfeecedc790faf6a6b27fe87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 496 ms, total: 28.3 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_name = {}\n",
    "cnt = 1\n",
    "for i in list(tokenizer.word_index.keys()):\n",
    "    if i in name:\n",
    "        token_name[i] = cnt\n",
    "        cnt += 1\n",
    "    else:\n",
    "        token_name[i] = 0\n",
    "\n",
    "tokenizer.word_index = token_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "ee2f6f331a8784887fbd076dea27a05625bebb55"
   },
   "outputs": [],
   "source": [
    "X_train = train_df['transform'].values\n",
    "X_val = val_df['transform'].values\n",
    "y_train = train_df['target'].values\n",
    "y_val = val_df['target'].values\n",
    "X_test = test_df['transform'].values\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train_name = sequence.pad_sequences(X_train, maxlen=maxlen, padding='post',truncating='post')\n",
    "x_val_name = sequence.pad_sequences(X_val, maxlen=maxlen, padding='post', truncating='post')\n",
    "x_test_name = sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "fde11e1c2c43c4dc64df9ed9e2e3ea50168ae150"
   },
   "outputs": [],
   "source": [
    "train_df['token'] = list(x_train)\n",
    "train_df['token_name'] = list(x_train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "50b43b17310126b5067c970c8a044b9e65828403"
   },
   "outputs": [],
   "source": [
    "mask_zeros = np.ones((name.shape[0] + 1, 600))\n",
    "mask_zeros[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "798c303ec834fb530a60a1e590cfbd9a86f93fde"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, Dense, Bidirectional, Input, SpatialDropout1D,Embedding, \\\n",
    "        BatchNormalization, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, Conv1D, Multiply, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6d8e00f54ef80ebafda2f48b8c8c7718cb184aba"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras import backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7105d08058d0d99aab6ae91ff7ffa43a339da49a"
   },
   "outputs": [],
   "source": [
    "class GlobalMinPooling1D(Layer):\n",
    "    \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.min(x, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         #return input_shape[0], input_shape[-1]\n",
    "#         return input_shape[0],  self.features_dim\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "7ab3cb0007d45f9a4288ccd0e5a008a3bf0a3553"
   },
   "outputs": [],
   "source": [
    "class GlobalSumPooling1D(Layer):\n",
    "    \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.sum(x, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2])\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         #return input_shape[0], input_shape[-1]\n",
    "#         return input_shape[0],  self.features_dim\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "e06dd1f2a0aeca63620ddf95ef8d5da2670e0b74"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.initializers import he_uniform\n",
    "def build_model(input_layer, input_layer_name,  embedding_matrix):\n",
    "    x1 = Embedding(embedding_matrix.shape[0], 600, weights=[embedding_matrix], trainable= False)(input_layer)\n",
    "    x2 = Embedding(name.shape[0] + 1, 600,  trainable= True)(input_layer_name)\n",
    "    x3 = Embedding(name.shape[0] + 1, 600,  weights=[mask_zeros], trainable= False)(input_layer_name)\n",
    "    #x = SpatialDropout1D(0.2)(x)\n",
    "    x = Multiply()([x2, x3])\n",
    "    x = Add()([x1, x])\n",
    "    x = Bidirectional(CuDNNLSTM(128, kernel_initializer=he_uniform(seed=0), return_sequences=True))(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    y = Bidirectional(CuDNNGRU(128,kernel_initializer=he_uniform(seed=0), return_sequences=True))(x)\n",
    "    a = GlobalAveragePooling1D()(y)\n",
    "    b = GlobalMaxPooling1D()(y)\n",
    "    c = GlobalMinPooling1D()(y)\n",
    "    #t = GlobalMaxPooling1D()(x)\n",
    "    #d = Attention(30)(x)\n",
    "    #e = Attention(30)(y)\n",
    "    x = concatenate([a, b])\n",
    "    x = Dense(32, activation=\"relu\",kernel_initializer=he_uniform(seed=0))(x)\n",
    "    x = Dense(1, activation=\"sigmoid\",kernel_initializer=he_uniform(seed=0))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "52db456d253ffb3eb366e3fe74d790d542b88667"
   },
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "04ff7707433d17da5025a94b0677127397cd6c6d"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "val_df.reset_index(drop= True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "3d516d03c2204ef0d8ce547f83ef2ea1e9a65ec2"
   },
   "outputs": [],
   "source": [
    "positive = train_df[train_df['target'] == 1][['token','token_name','target']]\n",
    "negative = train_df[train_df['target'] == 0][['token','token_name','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "e06dd1f2a0aeca63620ddf95ef8d5da2670e0b74",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 692777 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 400s - loss: 0.1537 - acc: 0.9380 - val_loss: 0.1019 - val_acc: 0.9617\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 505s - loss: 0.0932 - acc: 0.9620 - val_loss: 0.0979 - val_acc: 0.9610\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 372s - loss: 0.0840 - acc: 0.9655 - val_loss: 0.0956 - val_acc: 0.9594\n",
      "Train on 692777 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 401s - loss: 0.1532 - acc: 0.9385 - val_loss: 0.1010 - val_acc: 0.9610\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 509s - loss: 0.0933 - acc: 0.9621 - val_loss: 0.0972 - val_acc: 0.9610\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 373s - loss: 0.0840 - acc: 0.9656 - val_loss: 0.0989 - val_acc: 0.9602\n",
      "Train on 692777 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 401s - loss: 0.1539 - acc: 0.9377 - val_loss: 0.1033 - val_acc: 0.9579\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 509s - loss: 0.0935 - acc: 0.9620 - val_loss: 0.0965 - val_acc: 0.9633\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 374s - loss: 0.0841 - acc: 0.9656 - val_loss: 0.0984 - val_acc: 0.9579\n",
      "Train on 692777 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 403s - loss: 0.1537 - acc: 0.9384 - val_loss: 0.1024 - val_acc: 0.9602\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 510s - loss: 0.0935 - acc: 0.9621 - val_loss: 0.0960 - val_acc: 0.9625\n",
      "Train on 1304815 samples, validate on 1307 samples\n",
      "Epoch 1/1\n",
      " - 374s - loss: 0.0842 - acc: 0.9656 - val_loss: 0.0946 - val_acc: 0.9633\n"
     ]
    }
   ],
   "source": [
    "for C in range(4):\n",
    "    if (C == 0):\n",
    "        embedding_matrix = np.concatenate([embedding_matrix1, embedding_matrix2], axis=1)\n",
    "    elif (C == 1):\n",
    "        embedding_matrix = np.concatenate([embedding_matrix1, embedding_matrix2], axis=1)\n",
    "    elif (C == 2):\n",
    "        embedding_matrix = np.concatenate([embedding_matrix2, embedding_matrix1], axis=1)\n",
    "    else:\n",
    "        embedding_matrix = np.concatenate([ embedding_matrix2, embedding_matrix1], axis=1)\n",
    "\n",
    "    neg1, neg2 = train_test_split(negative, test_size = 0.5, random_state = C*100)\n",
    "    df1, df2 = pd.concat([neg1,positive], ignore_index=True), pd.concat([neg2,positive], ignore_index=True)\n",
    "    input_layer = Input((50,), name=\"i1\")\n",
    "    input_layer_name = Input((50, ), name = \"i2\")\n",
    "    output_layer = build_model(input_layer, input_layer_name, embedding_matrix)\n",
    "    model = Model([input_layer, input_layer_name], output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    model.fit({\"i1\": np.stack(df1['token'].values), \"i2\": np.stack(df1['token_name'].values)}, \n",
    "        df1['target'], batch_size=128, verbose=2,shuffle=True,\\\n",
    "              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n",
    "    \n",
    "\n",
    "    \n",
    "    model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=256, verbose=2,shuffle=True,\\\n",
    "              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n",
    "    #model.fit(x_train, train_df['target'], batch_size=1024, verbose=1,shuffle=True,\\\n",
    "    #          epochs=1, validation_data=(x_val, val_df['target']), callbacks=callbacks_list)\n",
    "    #val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n",
    "    #test_df[C] = model.predict(x_test, batch_size=512).flatten()\n",
    "    #best_search = threshold_search(val_df['target'].values, val_df[C].values)\n",
    "    #print(best_search)\n",
    "    \n",
    "    #print(model.layers[2].get_weights()[0])\n",
    "    model.layers[2].trainable = False\n",
    "    #model.layers[7].trainable = False\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "#     model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=512, verbose=2,shuffle=True,\\\n",
    "#               epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n",
    "#     #model.fit(x_train, train_df['target'], batch_size=1024, verbose=1,shuffle=True,\\\n",
    "#     #          epochs=1, validation_data=(x_val, val_df['target']), callbacks=callbacks_list)\n",
    "#     val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n",
    "#     #test_df[C] = model.predict(x_test, batch_size=512).flatten()\n",
    "#     best_search = threshold_search(val_df['target'].values, val_df[C].values)\n",
    "#     print(best_search)\n",
    "    \n",
    "    model.fit({\"i1\":x_train, \"i2\": x_train_name}, train_df['target'], batch_size=512, verbose=2,shuffle=True,\\\n",
    "              epochs=1, validation_data=({\"i1\": x_val, \"i2\": x_val_name}, val_df['target']))\n",
    "   \n",
    "    #print(model.layers[2].get_weights()[0])\n",
    "    #val_df[C] = model.predict({\"i1\": x_val, \"i2\": x_val_name}, batch_size=1024).flatten()\n",
    "    test_df[C] = model.predict({\"i1\": x_test, \"i2\": x_test_name}, batch_size=1024).flatten()\n",
    "    #best_search = threshold_search(val_df['target'].values, val_df[C].values)\n",
    "    #print(best_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "d940e61963942fd4718cc3f87cb64b77b805eb56"
   },
   "outputs": [],
   "source": [
    "#val_df['preds'] = (val_df[0] + val_df[1] + val_df[2] + val_df[3])/4\n",
    "test_df['preds'] = (test_df[0] + test_df[1] + test_df[2] + test_df[3])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "3e6ed54def110c881f401c6f8a844752baedcfbd"
   },
   "outputs": [],
   "source": [
    "y_te = (test_df['preds'] > 0.35).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
