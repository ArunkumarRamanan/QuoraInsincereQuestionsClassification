{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os, re, gc, random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to suppress some matplotlib deprecation warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6d8153dfa07ce541703d06feaed43c058f3334b9"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "597eea9d6295671bd36a809b579d12777738a392"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data, vocab\n",
    "from torchtext.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f9ae58b80e7a871ca069fcefdce83d24c43948e5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "torchtext.vocab.tqdm = tqdm_notebook # Replace tqdm to tqdm_notebook in module torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b7c04a817b5ba68498dc9b30638605da891ba6c4"
   },
   "outputs": [],
   "source": [
    "path = \"../input\"\n",
    "emb_path = \"../input/embeddings\"\n",
    "n_folds = 5\n",
    "bs = 512\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6ddd98901408dbe7c7fb9efdb0ec17cecd511864"
   },
   "outputs": [],
   "source": [
    "seed = 7777\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1fd588f56c18a0993c7db729b7524f81dc016126"
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "461f84f65cf5452f7d47cd07d4e7da984529c5bf"
   },
   "outputs": [],
   "source": [
    "mispell_dict = {   \n",
    "    \"can't\" : \"can not\", \"tryin'\":\"trying\",\n",
    "    \"'m\": \" am\", \"'ll\": \" 'll\", \"'d\" : \" 'd'\", \"..\": \"  \",\".\": \" . \", \",\":\" , \",\n",
    "    \"'ve\" : \" have\", \"n't\": \" not\",\"'s\": \" 's\", \"'re\": \" are\", \"$\": \" $\",\"’\": \" ' \",\n",
    "    \"y'all\": \"you all\", 'metoo': 'me too',\n",
    "    'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "    'travelling': 'traveling', 'counselling': 'counseling',\n",
    "    'centerd': 'centered',\n",
    "    'theatre': 'theater','cancelled': 'canceled','labour': 'labor',\n",
    "    'organisation': 'organization','wwii': 'world war 2', 'citicise': 'criticize',\n",
    "    'youtu ': 'youtube ','Qoura': 'Quora','sallary': 'salary','Whta': 'What',\n",
    "    'narcisist': 'narcissist','howdo': 'how do','whatare': 'what are',\n",
    "    'howcan': 'how can','howmuch': 'how much','howmany': 'how many', 'whydo': 'why do',\n",
    "    'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'doI': 'do I',\n",
    "    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', \n",
    "    '2k17': '2017', '2k18': '2018','qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
    "    'airhostess': 'air hostess', \"whst\": 'what', 'watsapp':'whatsapp',\n",
    "    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "    'demonetisation': 'demonetization','bigdata': 'big data',\n",
    "    'Quorans': 'Questions','quorans': 'questions','quoran':'question','Quoran':'Question',\n",
    "    # Slang and  abbreviation\n",
    "    'Skripal':'russian spy','Doklam':'Tibet', \n",
    "    'BNBR':'Be Nice Be Respectful', 'Brexit': 'British exit',\n",
    "    'Bhakts':'fascists','bhakts':'fascists','Bhakt':'fascist','bhakt':'fascist',\n",
    "    'SJWs':'Social Justice Warrior','SJW':'Social Justice Warrior',\n",
    "    'Modiji':'Prime Minister of India', 'Ra apist': 'Rapist', ' apist ':' ape ',\n",
    "    'wumao':'commenters','cucks': 'cuck', 'Strzok':'stupid phrase','strzok':'stupid phrase',\n",
    "    \n",
    "    ' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA',\n",
    "    'u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
    "    ' U.s ': 'USA', ' u.S ': ' USA ', ' fu.k': ' fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n",
    "    ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "    'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "    '2fifth': 'twenty fifth', '2third': 'twenty third',\n",
    "    '2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
    "    'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "    'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','culturr': 'culture',\n",
    "    'fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "    'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "    'weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "    'indans': 'indians', 'mastuburate': 'masturbate', ' f**k': ' fuck', ' F**k': ' fuck', ' F**K': ' fuck',\n",
    "    ' u r ': ' you are ', ' u ': ' you ', '操你妈 ': 'fuck your mother', ' e.g.': ' for example',\n",
    "    'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "    ' f***': ' fuck', ' f**': ' fuc', ' F***': ' fuck', ' F**': ' fuck',\n",
    "    ' a****': ' assho', 'a**': 'ass', ' h***': ' hole', 'A****': 'assho', ' A**': ' ass', ' H***': ' hole',\n",
    "    ' s***': ' shit', ' s**': 'shi', ' S***': ' shit', ' S**': ' shi', ' Sh**': 'shit',\n",
    "    ' p****': ' pussy', ' p*ssy': ' pussy', ' P****': ' pussy',\n",
    "    ' p***': ' porn', ' p*rn': ' porn', ' P***': ' porn',' Fck': ' Fuck',' fck': ' fuck',  \n",
    "    ' st*up*id': ' stupid', ' d***': 'dick', ' di**': ' dick', ' h*ck': ' hack',\n",
    "    ' b*tch': ' bitch', 'bi*ch': ' bitch', ' bit*h': ' bitch', ' bitc*': ' bitch', ' b****': ' bitch',\n",
    "    ' b***': ' bitc', ' b**': ' bit', ' b*ll': ' bull',' FATF': 'Western summit conference',\n",
    "    'Terroristan': 'terrorist Pakistan', 'terroristan': 'terrorist Pakistan',\n",
    "    ' incel': ' involuntary celibates', ' incels': ' involuntary celibates', 'emiratis': 'Emiratis',\n",
    "    'weatern': 'western', 'westernise': 'westernize', 'Pizzagate': 'debunked conspiracy theory', 'naïve': 'naive',\n",
    "    ' HYPSM': ' Harvard, Yale, Princeton, Stanford, MIT', ' HYPS': ' Harvard, Yale, Princeton, Stanford',\n",
    "    'kompromat': 'compromising material', ' Tharki': ' pervert', ' tharki': 'pervert',\n",
    "    'Naxali ': 'Naxalite ', 'Naxalities': 'Naxalites','Mewani': 'Indian politician Jignesh Mevani', ' Wjy': ' Why',\n",
    "    'Fadnavis': 'Indian politician Devendra Fadnavis', 'Awadesh': 'Indian engineer Awdhesh Singh',\n",
    "    'Awdhesh': 'Indian engineer Awdhesh Singh', 'Khalistanis': 'Sikh separatist movement',\n",
    "    'madheshi': 'Madheshi','Stupead': 'stupid',  'narcissit': 'narcissist',\n",
    "}\n",
    "\n",
    "def correct_spelling(s, dic):\n",
    "    for key, corr in dic.items():\n",
    "        s = s.replace(key, dic[key])\n",
    "    return s\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9!.,?$\\'\\\"]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text #.lower()\n",
    "\n",
    "def tokenizer(s): \n",
    "    s = correct_spelling(s, mispell_dict)\n",
    "    s = tweet_clean(s)\n",
    "    return tknzr.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7f1a53a68e48048e6383bca70f5b58ad82b8058a"
   },
   "outputs": [],
   "source": [
    "def find_threshold(y_t, y_p, floor=-1., ceil=1., steps=41):\n",
    "    thresholds = np.linspace(floor, ceil, steps)\n",
    "    best_val = 0.0\n",
    "    for threshold in thresholds:\n",
    "        val_predict = (y_p > threshold)\n",
    "        score = f1_score(y_t, val_predict)\n",
    "        if score > best_val:\n",
    "            best_threshold = threshold\n",
    "            best_val = score\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "bbfa9dfccdac8100e265eeb0038a6bcea121c677"
   },
   "outputs": [],
   "source": [
    "def splits_cv(data, cv, y=None):\n",
    "    \n",
    "    for indices in cv.split(range(len(data)), y):\n",
    "        (train_data, val_data) = tuple([data.examples[i] for i in index] for index in indices)\n",
    "        yield tuple(Dataset(d, data.fields) for d in (train_data, val_data) if d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c1acb71152dd8704c3e95725be2943c75e4a2561"
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n",
    "\n",
    "scores = pd.read_csv('../input/train.csv')\n",
    "target = scores.target.values\n",
    "scores = scores.set_index('qid')\n",
    "scores.drop(columns=['question_text'], inplace=True)\n",
    "\n",
    "subm = pd.read_csv('../input/test.csv')\n",
    "subm = subm.set_index('qid')\n",
    "subm.drop(columns='question_text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9640b7fef754155e0162b2cd1e70cadf94c6ec6c"
   },
   "outputs": [],
   "source": [
    "# define the columns that we want to process and how to process\n",
    "txt_field = data.Field(sequential=True, tokenize=tokenizer, include_lengths=False,  use_vocab=True)\n",
    "label_field = data.Field(sequential=False, use_vocab=False, is_target=True)\n",
    "qid_field = data.RawField()\n",
    "\n",
    "train_fields = [\n",
    "    ('qid', qid_field), # we dont need this, so no processing\n",
    "    ('question_text', txt_field), # process it as text\n",
    "    ('target', label_field) # process it as label\n",
    "]\n",
    "test_fields = [\n",
    "    ('qid', qid_field), \n",
    "    ('question_text', txt_field), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "92ea577a18faedfdccf5198d626c5c27861133ee"
   },
   "outputs": [],
   "source": [
    "# Loading csv file\n",
    "train_ds = data.TabularDataset(path=os.path.join(path, 'train.csv'), \n",
    "                           format='csv',\n",
    "                           fields=train_fields, \n",
    "                           skip_header=True)\n",
    "\n",
    "test_ds = data.TabularDataset(path=os.path.join(path, 'test.csv'), \n",
    "                           format='csv',\n",
    "                           fields=test_fields, \n",
    "                           skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "211c926c66b7429f40bf8cf327b51c8bd6bc7ba3"
   },
   "outputs": [],
   "source": [
    "test_ds.fields['qid'].is_target = False\n",
    "train_ds.fields['qid'].is_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "8e5feaed1764517dbb5e51c21f09c939dbb77704"
   },
   "outputs": [],
   "source": [
    "test_loader = data.BucketIterator(test_ds, batch_size=bs, device='cuda',\n",
    "                                sort_key=lambda x: len(x.question_text),\n",
    "                                sort_within_batch=True, \n",
    "                                shuffle=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "e64a4a40f2e23749d937004f00db7210cdd3d946",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RecNN(nn.Module):\n",
    "    def __init__(self, embs_vocab, hidden_size, layers=1, dropout=0., bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = layers\n",
    "        self.emb = nn.Embedding(embs_vocab.size(0), embs_vocab.size(1))\n",
    "        self.emb.weight.data.copy_(embs_vocab) # load pretrained vectors\n",
    "        self.emb.weight.requires_grad = False # make embedding non trainable\n",
    "        \n",
    "        self.line = nn.Conv1d(300, 300, 1, bias=True) # Linear transformation of embedding vectors\n",
    "        \n",
    "        self.lstm = nn.LSTM(embs_vocab.size(1), self.hidden_size,\n",
    "                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(embs_vocab.size(1), self.hidden_size,\n",
    "                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size*(bidirectional + 1), 32)\n",
    "        self.last = nn.Linear(32, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embs = self.emb(x)\n",
    "        lstm, (h, c) = self.lstm(embs)\n",
    "        \n",
    "        x = F.relu(self.line(embs.permute(1,2,0)), inplace=True).permute(2,0,1)\n",
    "        gru, h = self.gru(x, h)\n",
    "        lstm = lstm + gru\n",
    "        \n",
    "        lstm, _ = lstm.max(dim=0, keepdim=False) \n",
    "        out = self.out(lstm)\n",
    "        out = self.last(F.relu(out)).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e566ecdb5f44cf32af12127f073021566d6e66dd"
   },
   "outputs": [],
   "source": [
    "def OOF_preds(test_df, target, embs_vocab, epochs = 4, alias='prediction', cv=skf,\n",
    "              loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean', pos_weight=(torch.Tensor([2.7])).to(device)),\n",
    "              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64):\n",
    "\n",
    "    print('Embedding vocab size: ', embs_vocab.size()[0])\n",
    "    \n",
    "    test_df[alias] = 0.\n",
    "    \n",
    "    for train, _ in splits_cv(train_ds, cv, target):\n",
    "        \n",
    "        train = data.BucketIterator(train, batch_size=bs, device=device,\n",
    "                        sort_key=lambda x: len(x.question_text),\n",
    "                        sort_within_batch=True,\n",
    "                        shuffle=True, repeat=False)\n",
    "\n",
    "        model = RecNN(embs_vocab, n_hidden, dropout=0., bidirectional=bidirectional).to(device)\n",
    "        \n",
    "        opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3,\n",
    "                         betas=(0.75, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
    "        \n",
    "        print('\\n')\n",
    "        for epoch in range(epochs):      \n",
    "            y_true_train = np.empty(0)\n",
    "            y_pred_train = np.empty(0)\n",
    "            total_loss_train = 0          \n",
    "            model.train()\n",
    "            for (_, x), y in train:\n",
    "                y = y.type(dtype=torch.cuda.FloatTensor)\n",
    "                opt.zero_grad()\n",
    "                pred = model(x)\n",
    "                loss = loss_fn(pred, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                y_true_train = np.concatenate([y_true_train, y.cpu().data.numpy()], axis = 0)\n",
    "                y_pred_train = np.concatenate([y_pred_train, pred.cpu().squeeze().data.numpy()], axis = 0)\n",
    "                total_loss_train += loss.item()\n",
    "\n",
    "            tacc = f1_score(y_true_train, y_pred_train>0)\n",
    "            tloss = total_loss_train/len(train)\n",
    "            print(f'Epoch {epoch+1}: Train loss: {tloss:.4f}, F1: {tacc:.4f}')\n",
    "        \n",
    "        # Get prediction for test set\n",
    "        preds = torch.empty(0)\n",
    "        qids = []\n",
    "        for (y, x), _ in test_loader:\n",
    "            pred = model(x)\n",
    "            qids.append(y)\n",
    "            preds = torch.cat([preds, pred.detach().cpu()])\n",
    "            \n",
    "        # Save prediction of test set\n",
    "        preds = torch.sigmoid(preds).numpy()\n",
    "        qids = [item for sublist in qids for item in sublist]\n",
    "        test_df.at[qids, alias]  =  test_df.loc[qids][alias].values + preds/n_folds\n",
    "        \n",
    "        gc.enable();\n",
    "        del train\n",
    "        gc.collect();\n",
    "        \n",
    "    gc.enable();\n",
    "    del embs_vocab, model\n",
    "    gc.collect();     \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cb4c7f88da6b9c608a923f6119d5fac70cab7ffa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39d75a3c1a4fe1ab5c7dc49027d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=999994), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728d63aebb724197b6f2749577d67cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196017), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding loaded, vocab size:  240814\n"
     ]
    }
   ],
   "source": [
    "def preload_gnews():\n",
    "    # Google..... bin file.......\n",
    "    vector_google = KeyedVectors.load_word2vec_format(os.path.join(emb_path, embs_file['gnews']), binary=True)\n",
    "\n",
    "    stoi = {s:idx for idx, s in enumerate(vector_google.index2word)}\n",
    "    itos = {idx:s for idx, s in enumerate(vector_google.index2word)}\n",
    "\n",
    "    cache='cache/'\n",
    "    path_cache = os.path.join(cache, 'GoogleNews-vectors-negative300.bin')\n",
    "    file_suffix = '.pt'\n",
    "    path_pt = path_cache + file_suffix\n",
    "\n",
    "    torch.save((itos, stoi, torch.from_numpy(vector_google.vectors), vector_google.vectors.shape[1]), path_pt)\n",
    "\n",
    "    \n",
    "embs_file = {}\n",
    "embs_file['wiki'] = 'wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "embs_file['gnews'] = 'GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embs_file['glove'] = 'glove.840B.300d/glove.840B.300d.txt'\n",
    "embs_file['gram'] = 'paragram_300_sl999/paragram_300_sl999.txt'\n",
    "\n",
    "embs_vocab = {}\n",
    "\n",
    "!mkdir cache\n",
    "preload_gnews()\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.Vectors(os.path.join(emb_path, embs_file['gnews']), cache='cache/')\n",
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "txt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\n",
    "embs_vocab['gnews'] = train_ds.fields['question_text'].vocab.vectors\n",
    "!rm -r cache\n",
    "\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.Vectors(os.path.join(emb_path, embs_file['wiki']), cache='cache/')\n",
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "txt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\n",
    "embs_vocab['wiki'] = train_ds.fields['question_text'].vocab.vectors\n",
    "\n",
    "# specify the path to the localy saved vectors\n",
    "vec = vocab.Vectors(os.path.join(emb_path, embs_file['glove']), cache='cache/')\n",
    "# build the vocabulary using train and validation dataset and assign the vectors\n",
    "txt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\n",
    "embs_vocab['glove'] = train_ds.fields['question_text'].vocab.vectors\n",
    "\n",
    "print('Embedding loaded, vocab size: ', embs_vocab['glove'].size()[0])\n",
    "!rm -r cache\n",
    "gc.enable()\n",
    "del vec\n",
    "gc.collect(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6d9580087b3087d928afd4ead24aab495db2f6d6"
   },
   "outputs": [],
   "source": [
    "def fill_unknown(vector):\n",
    "    # fill from Glove\n",
    "    data = torch.zeros_like(vector)\n",
    "    data.copy_(vector)\n",
    "    idx = torch.nonzero(data.sum(dim=1) == 0)\n",
    "    data[idx] = embs_vocab['glove'][idx]\n",
    "    # fill from Wiki\n",
    "    idx = torch.nonzero(data.sum(dim=1) == 0)\n",
    "    data[idx] = embs_vocab['wiki'][idx]\n",
    "    # fill from GoogleNews\n",
    "    idx = torch.nonzero(data.sum(dim=1) == 0)\n",
    "    data[idx] = embs_vocab['gnews'][idx]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "11f59c3d8c209dc02d8a36a845f7c510e74dd6cc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vocab size:  240814\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2093, F1: 0.6216\n",
      "Epoch 2: Train loss: 0.1781, F1: 0.6701\n",
      "Epoch 3: Train loss: 0.1668, F1: 0.6869\n",
      "Epoch 4: Train loss: 0.1572, F1: 0.6999\n",
      "Epoch 5: Train loss: 0.1480, F1: 0.7126\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2101, F1: 0.6198\n",
      "Epoch 2: Train loss: 0.1787, F1: 0.6691\n",
      "Epoch 3: Train loss: 0.1667, F1: 0.6866\n",
      "Epoch 4: Train loss: 0.1565, F1: 0.6993\n",
      "Epoch 5: Train loss: 0.1472, F1: 0.7117\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2106, F1: 0.6192\n",
      "Epoch 2: Train loss: 0.1790, F1: 0.6691\n",
      "Epoch 3: Train loss: 0.1677, F1: 0.6852\n",
      "Epoch 4: Train loss: 0.1581, F1: 0.6975\n",
      "Epoch 5: Train loss: 0.1489, F1: 0.7109\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2116, F1: 0.6138\n",
      "Epoch 2: Train loss: 0.1794, F1: 0.6681\n",
      "Epoch 3: Train loss: 0.1679, F1: 0.6855\n",
      "Epoch 4: Train loss: 0.1583, F1: 0.6982\n",
      "Epoch 5: Train loss: 0.1490, F1: 0.7115\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2082, F1: 0.6237\n",
      "Epoch 2: Train loss: 0.1778, F1: 0.6689\n",
      "Epoch 3: Train loss: 0.1663, F1: 0.6862\n",
      "Epoch 4: Train loss: 0.1566, F1: 0.7003\n",
      "Epoch 5: Train loss: 0.1472, F1: 0.7128\n",
      "CPU times: user 22min 7s, sys: 6min 29s, total: 28min 37s\n",
      "Wall time: 28min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subm = OOF_preds(subm, target, epochs = 5, alias='wiki',\n",
    "#               loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean'), \n",
    "              embs_vocab=fill_unknown(embs_vocab['wiki']),\n",
    "              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed),\n",
    "              embedding_dim = 300, bidirectional=True, n_hidden = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "991cb7359d8d497835f6677910329233e6750861",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vocab size:  240814\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2009, F1: 0.6325\n",
      "Epoch 2: Train loss: 0.1728, F1: 0.6763\n",
      "Epoch 3: Train loss: 0.1600, F1: 0.6957\n",
      "Epoch 4: Train loss: 0.1481, F1: 0.7125\n",
      "Epoch 5: Train loss: 0.1357, F1: 0.7310\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.1997, F1: 0.6332\n",
      "Epoch 2: Train loss: 0.1722, F1: 0.6762\n",
      "Epoch 3: Train loss: 0.1589, F1: 0.6966\n",
      "Epoch 4: Train loss: 0.1475, F1: 0.7116\n",
      "Epoch 5: Train loss: 0.1351, F1: 0.7298\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.1992, F1: 0.6371\n",
      "Epoch 2: Train loss: 0.1714, F1: 0.6786\n",
      "Epoch 3: Train loss: 0.1592, F1: 0.6966\n",
      "Epoch 4: Train loss: 0.1473, F1: 0.7129\n",
      "Epoch 5: Train loss: 0.1350, F1: 0.7308\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.1986, F1: 0.6353\n",
      "Epoch 2: Train loss: 0.1716, F1: 0.6788\n",
      "Epoch 3: Train loss: 0.1585, F1: 0.6974\n",
      "Epoch 4: Train loss: 0.1464, F1: 0.7153\n",
      "Epoch 5: Train loss: 0.1341, F1: 0.7317\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2002, F1: 0.6327\n",
      "Epoch 2: Train loss: 0.1723, F1: 0.6785\n",
      "Epoch 3: Train loss: 0.1596, F1: 0.6969\n",
      "Epoch 4: Train loss: 0.1478, F1: 0.7132\n",
      "Epoch 5: Train loss: 0.1350, F1: 0.7311\n",
      "CPU times: user 22min 26s, sys: 6min 25s, total: 28min 51s\n",
      "Wall time: 28min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subm = OOF_preds(subm, target, epochs = 5, alias='glove',\n",
    "              embs_vocab=fill_unknown(embs_vocab['glove']),\n",
    "              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed+15),\n",
    "              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "94ba7d9f06af2533780b352ef602351d40d0d762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vocab size:  240814\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2089, F1: 0.6188\n",
      "Epoch 2: Train loss: 0.1778, F1: 0.6695\n",
      "Epoch 3: Train loss: 0.1657, F1: 0.6868\n",
      "Epoch 4: Train loss: 0.1552, F1: 0.7000\n",
      "Epoch 5: Train loss: 0.1444, F1: 0.7165\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2119, F1: 0.6179\n",
      "Epoch 2: Train loss: 0.1797, F1: 0.6667\n",
      "Epoch 3: Train loss: 0.1675, F1: 0.6851\n",
      "Epoch 4: Train loss: 0.1574, F1: 0.6983\n",
      "Epoch 5: Train loss: 0.1472, F1: 0.7124\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2104, F1: 0.6188\n",
      "Epoch 2: Train loss: 0.1798, F1: 0.6671\n",
      "Epoch 3: Train loss: 0.1675, F1: 0.6842\n",
      "Epoch 4: Train loss: 0.1568, F1: 0.6985\n",
      "Epoch 5: Train loss: 0.1460, F1: 0.7141\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2111, F1: 0.6172\n",
      "Epoch 2: Train loss: 0.1785, F1: 0.6683\n",
      "Epoch 3: Train loss: 0.1663, F1: 0.6856\n",
      "Epoch 4: Train loss: 0.1564, F1: 0.6995\n",
      "Epoch 5: Train loss: 0.1457, F1: 0.7146\n",
      "\n",
      "\n",
      "Epoch 1: Train loss: 0.2077, F1: 0.6240\n",
      "Epoch 2: Train loss: 0.1783, F1: 0.6684\n",
      "Epoch 3: Train loss: 0.1662, F1: 0.6863\n",
      "Epoch 4: Train loss: 0.1557, F1: 0.7003\n",
      "Epoch 5: Train loss: 0.1451, F1: 0.7149\n",
      "CPU times: user 22min 32s, sys: 6min 22s, total: 28min 54s\n",
      "Wall time: 28min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subm = OOF_preds(subm, target, epochs = 5, alias='gnews',\n",
    "              embs_vocab=fill_unknown(embs_vocab['gnews']),\n",
    "              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed+25),\n",
    "              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "c2d3a79fb61ae30dc46ff201110cec6bbabf2c89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki</th>\n",
       "      <th>glove</th>\n",
       "      <th>gnews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wiki</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974964</td>\n",
       "      <td>0.979182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glove</th>\n",
       "      <td>0.974964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gnews</th>\n",
       "      <td>0.979182</td>\n",
       "      <td>0.972538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           wiki     glove     gnews\n",
       "wiki   1.000000  0.974964  0.979182\n",
       "glove  0.974964  1.000000  0.972538\n",
       "gnews  0.979182  0.972538  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "419491d50aed847a9047c8583bf094d1a38fbf92"
   },
   "outputs": [],
   "source": [
    "submission = np.mean(subm.values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7c6a41bd21bf783455fd11b63745ed454e0413f0"
   },
   "outputs": [],
   "source": [
    "subm['prediction'] = submission > 0.55\n",
    "subm.prediction = subm.prediction.astype('int')\n",
    "subm.to_csv('submission.csv', columns=['prediction'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
